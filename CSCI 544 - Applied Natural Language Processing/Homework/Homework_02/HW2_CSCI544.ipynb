{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import numpy\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "from gensim import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up cuda environment to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#print(torch.cuda.is_available(), torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_13840\\4051677504.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pandas.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "#importing data as dataframe\n",
    "#data = pandas.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip')\n",
    "data = pandas.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove null value rows and reset index\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "#Keep only review_body column and corresponding star_rating column\n",
    "data = data[['review_body', 'star_rating']]\n",
    "\n",
    "#Removing all non-integer star_rating\n",
    "data['star_rating'] = data['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 100000 having 20000 from each star_rating class\n",
    "data_1 = data[data['star_rating'] == 1].sample(n = 20000, random_state = 1)\n",
    "data_2 = data[data['star_rating'] == 2].sample(n = 20000, random_state = 1)\n",
    "data_3 = data[data['star_rating'] == 3].sample(n = 20000, random_state = 1)\n",
    "data_4 = data[data['star_rating'] == 4].sample(n = 20000, random_state = 1)\n",
    "data_5 = data[data['star_rating'] == 5].sample(n = 20000, random_state = 1)\n",
    "dataset = pandas.concat([data_1, data_2, data_3, data_4, data_5])\n",
    "\n",
    "#print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pre-trained model\n",
    "model_pre_trained = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between excellent and outstanding =  0.55674857\n",
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "#Check the similarity between two similar words\n",
    "print('Similarity between excellent and outstanding = ', model_pre_trained.similarity('excellent', 'outstanding'))\n",
    "\n",
    "#Find the corresponding word given that king - man + woman = queen \n",
    "print(model_pre_trained.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training my own Word2Vec Model\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in dataset['review_body']:\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading my own Word2Vec Model\n",
    "model_own_trained = models.Word2Vec(sentences = MyCorpus(), vector_size = 300, window = 11, min_count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between excellent and outstanding =  0.78527915\n",
      "[('avenue', 0.5512246489524841)]\n"
     ]
    }
   ],
   "source": [
    "#Check the similarity between two similar words\n",
    "print('Similarity between excellent and outstanding = ', model_own_trained.wv.similarity('excellent', 'outstanding'))\n",
    "\n",
    "#Find the corresponding word given that king - man + woman = queen \n",
    "print(model_own_trained.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model was trained by a much larger corpus (in terms of both quantities and varieties) than my own training dataset. Therefore, the pretrained model encodes better similar words between words than my own model. But since it has such a vast corpus, my model encodes better similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the mean of Word2Vec vectors for each review as the input feature\n",
    "\n",
    "def word2vec_mean(sentences, model):\n",
    "    sentence_split = sentences.split(' ')\n",
    "    sum = numpy.zeros(shape = (300,))\n",
    "    count = 0\n",
    "\n",
    "    for word in sentence_split:\n",
    "        if word in model:\n",
    "            word_vector = model[word]\n",
    "            sum += word_vector\n",
    "            count += 1\n",
    "    mean_vector = sum/count\n",
    "\n",
    "    return mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to concatenate the first 10 Word2Vec vectors for each review as the input feature\n",
    "\n",
    "def word2vec_concatenation(sentence, model):\n",
    "\n",
    "  ls = []\n",
    "  if type(sentence) == list:\n",
    "    ls = sentence\n",
    "  if type(sentence) == str:\n",
    "    ls = sentence.split(' ')\n",
    "\n",
    "  i = 0 \n",
    "  j = 0 \n",
    "\n",
    "  while (i < len(ls)) & (j < 10):\n",
    "    if ls[i] in model:\n",
    "      wv = model[ls[i]]\n",
    "      if j == 0:\n",
    "        a = wv\n",
    "      else:\n",
    "        a = numpy.concatenate((a, wv))\n",
    "      i += 1\n",
    "      j += 1\n",
    "    else:\n",
    "      i += 1\n",
    "      \n",
    "  if j < 10:\n",
    "    n = 10 - j\n",
    "    zeros = numpy.zeros(shape=(300*n, ))\n",
    "    if n == 10:\n",
    "      a = zeros\n",
    "    else:\n",
    "      a = numpy.concatenate((a, zeros))\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to truncate longer reviews to the length of 20 and padding smaller reviews\n",
    "\n",
    "def word2vec_sequence(sentence, model):\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        ls = sentence.split(' ')\n",
    "    if type(sentence) == list:\n",
    "        ls = sentence\n",
    "\n",
    "    word_vector = []\n",
    "    for i in range(20):\n",
    "      try:\n",
    "        wv = model[ls[i]]\n",
    "        word_vector.append(wv)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "    if len(word_vector) < 20:\n",
    "      for _ in range(20-len(word_vector)):\n",
    "        word_vector.append([0 for _ in range(300)])\n",
    "        \n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the indexes of NaN in the dataset\n",
    "\n",
    "def idx_nan(matrix):\n",
    "  if numpy.any(numpy.isnan(matrix)):\n",
    "    arr_nan = numpy.argwhere(numpy.isnan(matrix))\n",
    "    num_nan = arr_nan.shape[0]\n",
    "    arr = numpy.arange(0, num_nan, 300)\n",
    "    idx = []\n",
    "    for i in arr:\n",
    "      idx.append(arr_nan[i][0])\n",
    "    return idx\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and test dataset\n",
    "\n",
    "labels = dataset['star_rating'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['review_body'], labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting average of Word2Vec vectors as input for training and testing data and removing the NaN values\n",
    "\n",
    "X_train_average = X_train.apply(lambda x: word2vec_mean(x, model_pre_trained))\n",
    "X_train_pre_trained = numpy.array(X_train_average.values.tolist())\n",
    "\n",
    "X_test_average = X_test.apply(lambda x: word2vec_mean(x, model_pre_trained))\n",
    "X_test_pre_trained = numpy.array(X_test_average.values.tolist())\n",
    "\n",
    "idx_nan_train = idx_nan(X_train_pre_trained)\n",
    "if idx_nan_train == None:\n",
    "  y_train_pre_trained = y_train\n",
    "else:\n",
    "  X_train_pre_trained = numpy.delete(X_train_pre_trained, idx_nan_train, 0)\n",
    "  y_train_pre_trained = numpy.delete(y_train, idx_nan_train)\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_pre_trained)\n",
    "if idx_nan_test == None:\n",
    "  y_test_pre_trained = y_test\n",
    "else:\n",
    "  X_test_pre_trained = numpy.delete(X_test_pre_trained, idx_nan_test, 0)\n",
    "  y_test_pre_trained = numpy.delete(y_test, idx_nan_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy =  0.3560122606904176\n",
      "Perceptron Accuracy using TFIDF Feature Extraction = 0.40675\n"
     ]
    }
   ],
   "source": [
    "#Perceptron Analysis\n",
    "\n",
    "perceptron_pre_trained = Perceptron()\n",
    "perceptron_pre_trained.fit(X_train_pre_trained, y_train_pre_trained)\n",
    "\n",
    "prediction_perceptron_pre_trained = perceptron_pre_trained.predict(X_test_pre_trained)\n",
    "\n",
    "#print(metrics.classification_report(y_test_pre_trained, prediction_perceptron_pre_trained))\n",
    "print('Perceptron Accuracy = ', accuracy_score(y_test_pre_trained, prediction_perceptron_pre_trained))\n",
    "print('Perceptron Accuracy using TFIDF Feature Extraction = 0.40675')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy =  0.4649515099743731\n",
      "SVM Accuracy using TFIDF Feature Extraction = 0.4897\n"
     ]
    }
   ],
   "source": [
    "#SVM Analysis\n",
    "\n",
    "svm_pre_trained = LinearSVC()\n",
    "svm_pre_trained.fit(X_train_pre_trained, y_train_pre_trained)\n",
    "\n",
    "prediction_svm_pre_trained = svm_pre_trained.predict(X_test_pre_trained)\n",
    "\n",
    "#print(metrics.classification_report(y_test_pre_trained, prediction_svm_pre_trained))\n",
    "print('SVM Accuracy = ', accuracy_score(y_test_pre_trained, prediction_svm_pre_trained))\n",
    "print('SVM Accuracy using TFIDF Feature Extraction = 0.4897')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF model's performance is better than the Word2vec model because the number of data in each rating class is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting average of Word2Vec vectors as input for training and testing data and removing NaN Values\n",
    "\n",
    "X_train_nn = X_train.apply(lambda x: word2vec_mean(x, model_pre_trained))\n",
    "X_train_fnn = numpy.array(X_train_nn.values.tolist())\n",
    "\n",
    "X_test_nn = X_test.apply(lambda x: word2vec_mean(x, model_pre_trained))\n",
    "X_test_fnn = numpy.array(X_test_nn.values.tolist())\n",
    "\n",
    "idx_nan_train = idx_nan(X_train_fnn)\n",
    "if idx_nan_train != None:\n",
    "  X_train_fnn_pre_trained = numpy.delete(X_train_fnn, idx_nan_train, 0)\n",
    "  y_train_fnn_pre_trained = numpy.delete(y_train, idx_nan_train)\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_fnn)\n",
    "if idx_nan_test != None:\n",
    "  X_test_fnn_pre_trained = numpy.delete(X_test_fnn, idx_nan_test, 0)\n",
    "  y_test_fnn_pre_trained = numpy.delete(y_test, idx_nan_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the dataset classes\n",
    "\n",
    "class Train(Dataset):\n",
    "  def __init__(self, Xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = Xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class Test(Dataset):\n",
    "  def __init__(self, Xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = Xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training and testing dataset for the FNN Model\n",
    "\n",
    "train_fnn_pre_trained = Train(X_train_fnn_pre_trained, y_train_fnn_pre_trained - 1)\n",
    "test_fnn_pre_trained = Test(X_test_fnn_pre_trained, y_test_fnn_pre_trained - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching and Loading data for the FNN Model\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_fnn_pre_trained)\n",
    "indices = list(range(num_train))\n",
    "numpy.random.shuffle(indices)\n",
    "split = int(numpy.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader_fnn = torch.utils.data.DataLoader(train_fnn_pre_trained, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_fnn = torch.utils.data.DataLoader(train_fnn_pre_trained, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_fnn = torch.utils.data.DataLoader(test_fnn_pre_trained, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the MLP Architecture\n",
    "\n",
    "class ThreeLayerMLP(torch.nn.Module):\n",
    "  def __init__(self, D_in, H1, H2, D_out):\n",
    "    super().__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "    self.linear2 = torch.nn.Linear(H1, H2)\n",
    "    self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1_relu = torch.nn.functional.relu(self.linear1(x))\n",
    "    h1_drop = self.dropout(h1_relu)\n",
    "    h2_relu = torch.nn.functional.relu(self.linear2(h1_drop))\n",
    "    h2_drop = self.dropout(h2_relu)\n",
    "    h2_output = self.linear3(h2_drop)\n",
    "\n",
    "    return h2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initializing the FNN Model\n",
    "\n",
    "model_fnn = ThreeLayerMLP(300, 50, 10, 5)\n",
    "model_fnn.cuda()\n",
    "print(model_fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the parameters for the FNN Model\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_fnn = torch.optim.SGD(model_fnn.parameters(), lr=0.0065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.292683 \tValidation Loss: 0.322149\n",
      "Validation loss decreased (inf --> 0.322149).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.287957 \tValidation Loss: 0.321724\n",
      "Validation loss decreased (0.322149 --> 0.321724).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.286769 \tValidation Loss: 0.321563\n",
      "Validation loss decreased (0.321724 --> 0.321563).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.286202 \tValidation Loss: 0.321425\n",
      "Validation loss decreased (0.321563 --> 0.321425).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.285575 \tValidation Loss: 0.321263\n",
      "Validation loss decreased (0.321425 --> 0.321263).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.284920 \tValidation Loss: 0.321063\n",
      "Validation loss decreased (0.321263 --> 0.321063).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.284021 \tValidation Loss: 0.320808\n",
      "Validation loss decreased (0.321063 --> 0.320808).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.282907 \tValidation Loss: 0.320470\n",
      "Validation loss decreased (0.320808 --> 0.320470).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.281266 \tValidation Loss: 0.320012\n",
      "Validation loss decreased (0.320470 --> 0.320012).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.279242 \tValidation Loss: 0.319389\n",
      "Validation loss decreased (0.320012 --> 0.319389).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.276343 \tValidation Loss: 0.318522\n",
      "Validation loss decreased (0.319389 --> 0.318522).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.272664 \tValidation Loss: 0.317335\n",
      "Validation loss decreased (0.318522 --> 0.317335).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.267247 \tValidation Loss: 0.315706\n",
      "Validation loss decreased (0.317335 --> 0.315706).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.260388 \tValidation Loss: 0.313569\n",
      "Validation loss decreased (0.315706 --> 0.313569).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.251840 \tValidation Loss: 0.310971\n",
      "Validation loss decreased (0.313569 --> 0.310971).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.240352 \tValidation Loss: 0.307866\n",
      "Validation loss decreased (0.310971 --> 0.307866).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.229731 \tValidation Loss: 0.304679\n",
      "Validation loss decreased (0.307866 --> 0.304679).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.218021 \tValidation Loss: 0.301399\n",
      "Validation loss decreased (0.304679 --> 0.301399).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.206283 \tValidation Loss: 0.298267\n",
      "Validation loss decreased (0.301399 --> 0.298267).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.195711 \tValidation Loss: 0.295209\n",
      "Validation loss decreased (0.298267 --> 0.295209).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.185283 \tValidation Loss: 0.292364\n",
      "Validation loss decreased (0.295209 --> 0.292364).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.174848 \tValidation Loss: 0.289768\n",
      "Validation loss decreased (0.292364 --> 0.289768).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.165547 \tValidation Loss: 0.287433\n",
      "Validation loss decreased (0.289768 --> 0.287433).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.158442 \tValidation Loss: 0.285354\n",
      "Validation loss decreased (0.287433 --> 0.285354).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.151410 \tValidation Loss: 0.283955\n",
      "Validation loss decreased (0.285354 --> 0.283955).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.144692 \tValidation Loss: 0.281944\n",
      "Validation loss decreased (0.283955 --> 0.281944).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.138779 \tValidation Loss: 0.280462\n",
      "Validation loss decreased (0.281944 --> 0.280462).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.134241 \tValidation Loss: 0.279200\n",
      "Validation loss decreased (0.280462 --> 0.279200).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.130106 \tValidation Loss: 0.277950\n",
      "Validation loss decreased (0.279200 --> 0.277950).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.127007 \tValidation Loss: 0.276895\n",
      "Validation loss decreased (0.277950 --> 0.276895).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.122392 \tValidation Loss: 0.276117\n",
      "Validation loss decreased (0.276895 --> 0.276117).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.119478 \tValidation Loss: 0.274998\n",
      "Validation loss decreased (0.276117 --> 0.274998).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.116990 \tValidation Loss: 0.274190\n",
      "Validation loss decreased (0.274998 --> 0.274190).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.113678 \tValidation Loss: 0.273675\n",
      "Validation loss decreased (0.274190 --> 0.273675).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.110058 \tValidation Loss: 0.272737\n",
      "Validation loss decreased (0.273675 --> 0.272737).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.108701 \tValidation Loss: 0.272089\n",
      "Validation loss decreased (0.272737 --> 0.272089).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.103638 \tValidation Loss: 0.271326\n",
      "Validation loss decreased (0.272089 --> 0.271326).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.102357 \tValidation Loss: 0.270785\n",
      "Validation loss decreased (0.271326 --> 0.270785).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.100002 \tValidation Loss: 0.270159\n",
      "Validation loss decreased (0.270785 --> 0.270159).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.098822 \tValidation Loss: 0.269616\n",
      "Validation loss decreased (0.270159 --> 0.269616).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.096288 \tValidation Loss: 0.269166\n",
      "Validation loss decreased (0.269616 --> 0.269166).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.094419 \tValidation Loss: 0.268705\n",
      "Validation loss decreased (0.269166 --> 0.268705).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.092282 \tValidation Loss: 0.268455\n",
      "Validation loss decreased (0.268705 --> 0.268455).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.091669 \tValidation Loss: 0.267792\n",
      "Validation loss decreased (0.268455 --> 0.267792).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.089242 \tValidation Loss: 0.267594\n",
      "Validation loss decreased (0.267792 --> 0.267594).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.088324 \tValidation Loss: 0.267093\n",
      "Validation loss decreased (0.267594 --> 0.267093).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.086234 \tValidation Loss: 0.266759\n",
      "Validation loss decreased (0.267093 --> 0.266759).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.085818 \tValidation Loss: 0.266376\n",
      "Validation loss decreased (0.266759 --> 0.266376).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.084444 \tValidation Loss: 0.266090\n",
      "Validation loss decreased (0.266376 --> 0.266090).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.082550 \tValidation Loss: 0.265722\n",
      "Validation loss decreased (0.266090 --> 0.265722).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.081482 \tValidation Loss: 0.265723\n",
      "Epoch: 52 \tTraining Loss: 1.080244 \tValidation Loss: 0.265242\n",
      "Validation loss decreased (0.265722 --> 0.265242).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.078599 \tValidation Loss: 0.264837\n",
      "Validation loss decreased (0.265242 --> 0.264837).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.077605 \tValidation Loss: 0.264740\n",
      "Validation loss decreased (0.264837 --> 0.264740).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 1.078042 \tValidation Loss: 0.264763\n",
      "Epoch: 56 \tTraining Loss: 1.076281 \tValidation Loss: 0.264333\n",
      "Validation loss decreased (0.264740 --> 0.264333).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 1.075139 \tValidation Loss: 0.263955\n",
      "Validation loss decreased (0.264333 --> 0.263955).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.073100 \tValidation Loss: 0.263749\n",
      "Validation loss decreased (0.263955 --> 0.263749).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.072775 \tValidation Loss: 0.263475\n",
      "Validation loss decreased (0.263749 --> 0.263475).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 1.071944 \tValidation Loss: 0.263425\n",
      "Validation loss decreased (0.263475 --> 0.263425).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 1.072630 \tValidation Loss: 0.263028\n",
      "Validation loss decreased (0.263425 --> 0.263028).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 1.068958 \tValidation Loss: 0.262770\n",
      "Validation loss decreased (0.263028 --> 0.262770).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 1.069318 \tValidation Loss: 0.262996\n",
      "Epoch: 64 \tTraining Loss: 1.070361 \tValidation Loss: 0.262701\n",
      "Validation loss decreased (0.262770 --> 0.262701).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 1.068634 \tValidation Loss: 0.262788\n",
      "Epoch: 66 \tTraining Loss: 1.068793 \tValidation Loss: 0.262156\n",
      "Validation loss decreased (0.262701 --> 0.262156).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 1.066876 \tValidation Loss: 0.262064\n",
      "Validation loss decreased (0.262156 --> 0.262064).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 1.066148 \tValidation Loss: 0.261994\n",
      "Validation loss decreased (0.262064 --> 0.261994).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 1.064068 \tValidation Loss: 0.261625\n",
      "Validation loss decreased (0.261994 --> 0.261625).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 1.064984 \tValidation Loss: 0.261626\n",
      "Epoch: 71 \tTraining Loss: 1.063394 \tValidation Loss: 0.261444\n",
      "Validation loss decreased (0.261625 --> 0.261444).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 1.063067 \tValidation Loss: 0.261139\n",
      "Validation loss decreased (0.261444 --> 0.261139).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 1.063271 \tValidation Loss: 0.261161\n",
      "Epoch: 74 \tTraining Loss: 1.060673 \tValidation Loss: 0.260874\n",
      "Validation loss decreased (0.261139 --> 0.260874).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 1.061400 \tValidation Loss: 0.260873\n",
      "Validation loss decreased (0.260874 --> 0.260873).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 1.059747 \tValidation Loss: 0.261013\n",
      "Epoch: 77 \tTraining Loss: 1.058089 \tValidation Loss: 0.260536\n",
      "Validation loss decreased (0.260873 --> 0.260536).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 1.061170 \tValidation Loss: 0.260612\n",
      "Epoch: 79 \tTraining Loss: 1.059774 \tValidation Loss: 0.260480\n",
      "Validation loss decreased (0.260536 --> 0.260480).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 1.059196 \tValidation Loss: 0.260208\n",
      "Validation loss decreased (0.260480 --> 0.260208).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 1.057723 \tValidation Loss: 0.260164\n",
      "Validation loss decreased (0.260208 --> 0.260164).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 1.058541 \tValidation Loss: 0.259884\n",
      "Validation loss decreased (0.260164 --> 0.259884).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 1.056704 \tValidation Loss: 0.259836\n",
      "Validation loss decreased (0.259884 --> 0.259836).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 1.055401 \tValidation Loss: 0.259625\n",
      "Validation loss decreased (0.259836 --> 0.259625).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 1.055621 \tValidation Loss: 0.259706\n",
      "Epoch: 86 \tTraining Loss: 1.053810 \tValidation Loss: 0.259537\n",
      "Validation loss decreased (0.259625 --> 0.259537).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 1.055035 \tValidation Loss: 0.259391\n",
      "Validation loss decreased (0.259537 --> 0.259391).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 1.054015 \tValidation Loss: 0.259266\n",
      "Validation loss decreased (0.259391 --> 0.259266).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 1.054502 \tValidation Loss: 0.259310\n",
      "Epoch: 90 \tTraining Loss: 1.052920 \tValidation Loss: 0.258951\n",
      "Validation loss decreased (0.259266 --> 0.258951).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 1.052854 \tValidation Loss: 0.259224\n",
      "Epoch: 92 \tTraining Loss: 1.052481 \tValidation Loss: 0.258767\n",
      "Validation loss decreased (0.258951 --> 0.258767).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 1.050829 \tValidation Loss: 0.258568\n",
      "Validation loss decreased (0.258767 --> 0.258568).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 1.049902 \tValidation Loss: 0.258456\n",
      "Validation loss decreased (0.258568 --> 0.258456).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 1.050674 \tValidation Loss: 0.259121\n",
      "Epoch: 96 \tTraining Loss: 1.050916 \tValidation Loss: 0.258599\n",
      "Epoch: 97 \tTraining Loss: 1.050026 \tValidation Loss: 0.258504\n",
      "Epoch: 98 \tTraining Loss: 1.049605 \tValidation Loss: 0.258245\n",
      "Validation loss decreased (0.258456 --> 0.258245).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 1.048926 \tValidation Loss: 0.258481\n",
      "Epoch: 100 \tTraining Loss: 1.049392 \tValidation Loss: 0.258264\n",
      "Epoch: 101 \tTraining Loss: 1.048676 \tValidation Loss: 0.257874\n",
      "Validation loss decreased (0.258245 --> 0.257874).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 1.048245 \tValidation Loss: 0.257756\n",
      "Validation loss decreased (0.257874 --> 0.257756).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 1.047273 \tValidation Loss: 0.257948\n",
      "Epoch: 104 \tTraining Loss: 1.047355 \tValidation Loss: 0.258257\n",
      "Epoch: 105 \tTraining Loss: 1.047502 \tValidation Loss: 0.257798\n",
      "Epoch: 106 \tTraining Loss: 1.047132 \tValidation Loss: 0.257812\n",
      "Epoch: 107 \tTraining Loss: 1.046391 \tValidation Loss: 0.257924\n",
      "Epoch: 108 \tTraining Loss: 1.046128 \tValidation Loss: 0.257418\n",
      "Validation loss decreased (0.257756 --> 0.257418).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 1.045514 \tValidation Loss: 0.257394\n",
      "Validation loss decreased (0.257418 --> 0.257394).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 1.045223 \tValidation Loss: 0.257209\n",
      "Validation loss decreased (0.257394 --> 0.257209).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 1.045394 \tValidation Loss: 0.257142\n",
      "Validation loss decreased (0.257209 --> 0.257142).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 1.044657 \tValidation Loss: 0.256981\n",
      "Validation loss decreased (0.257142 --> 0.256981).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 1.043919 \tValidation Loss: 0.256992\n",
      "Epoch: 114 \tTraining Loss: 1.042604 \tValidation Loss: 0.256804\n",
      "Validation loss decreased (0.256981 --> 0.256804).  Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 1.043056 \tValidation Loss: 0.257114\n",
      "Epoch: 116 \tTraining Loss: 1.041420 \tValidation Loss: 0.256553\n",
      "Validation loss decreased (0.256804 --> 0.256553).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 1.041360 \tValidation Loss: 0.256827\n",
      "Epoch: 118 \tTraining Loss: 1.042787 \tValidation Loss: 0.256472\n",
      "Validation loss decreased (0.256553 --> 0.256472).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 1.041378 \tValidation Loss: 0.256289\n",
      "Validation loss decreased (0.256472 --> 0.256289).  Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 1.040441 \tValidation Loss: 0.256232\n",
      "Validation loss decreased (0.256289 --> 0.256232).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 1.040073 \tValidation Loss: 0.256087\n",
      "Validation loss decreased (0.256232 --> 0.256087).  Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 1.038948 \tValidation Loss: 0.255967\n",
      "Validation loss decreased (0.256087 --> 0.255967).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 1.040758 \tValidation Loss: 0.256038\n",
      "Epoch: 124 \tTraining Loss: 1.039508 \tValidation Loss: 0.255896\n",
      "Validation loss decreased (0.255967 --> 0.255896).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 1.038876 \tValidation Loss: 0.255997\n",
      "Epoch: 126 \tTraining Loss: 1.038370 \tValidation Loss: 0.255627\n",
      "Validation loss decreased (0.255896 --> 0.255627).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 1.038528 \tValidation Loss: 0.255552\n",
      "Validation loss decreased (0.255627 --> 0.255552).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 1.038741 \tValidation Loss: 0.255610\n",
      "Epoch: 129 \tTraining Loss: 1.037755 \tValidation Loss: 0.255528\n",
      "Validation loss decreased (0.255552 --> 0.255528).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 1.036565 \tValidation Loss: 0.255337\n",
      "Validation loss decreased (0.255528 --> 0.255337).  Saving model ...\n",
      "Epoch: 131 \tTraining Loss: 1.037083 \tValidation Loss: 0.255655\n",
      "Epoch: 132 \tTraining Loss: 1.037053 \tValidation Loss: 0.255408\n",
      "Epoch: 133 \tTraining Loss: 1.035254 \tValidation Loss: 0.255114\n",
      "Validation loss decreased (0.255337 --> 0.255114).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 1.035182 \tValidation Loss: 0.254829\n",
      "Validation loss decreased (0.255114 --> 0.254829).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 1.035491 \tValidation Loss: 0.254849\n",
      "Epoch: 136 \tTraining Loss: 1.035620 \tValidation Loss: 0.254774\n",
      "Validation loss decreased (0.254829 --> 0.254774).  Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 1.034208 \tValidation Loss: 0.254707\n",
      "Validation loss decreased (0.254774 --> 0.254707).  Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 1.034795 \tValidation Loss: 0.254556\n",
      "Validation loss decreased (0.254707 --> 0.254556).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 1.033384 \tValidation Loss: 0.254579\n",
      "Epoch: 140 \tTraining Loss: 1.032956 \tValidation Loss: 0.254326\n",
      "Validation loss decreased (0.254556 --> 0.254326).  Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 1.032364 \tValidation Loss: 0.254077\n",
      "Validation loss decreased (0.254326 --> 0.254077).  Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 1.030923 \tValidation Loss: 0.254313\n",
      "Epoch: 143 \tTraining Loss: 1.032406 \tValidation Loss: 0.254068\n",
      "Validation loss decreased (0.254077 --> 0.254068).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 1.031199 \tValidation Loss: 0.253808\n",
      "Validation loss decreased (0.254068 --> 0.253808).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 1.031064 \tValidation Loss: 0.253837\n",
      "Epoch: 146 \tTraining Loss: 1.029613 \tValidation Loss: 0.253888\n",
      "Epoch: 147 \tTraining Loss: 1.030681 \tValidation Loss: 0.253586\n",
      "Validation loss decreased (0.253808 --> 0.253586).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 1.031085 \tValidation Loss: 0.253675\n",
      "Epoch: 149 \tTraining Loss: 1.030082 \tValidation Loss: 0.253225\n",
      "Validation loss decreased (0.253586 --> 0.253225).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 1.028522 \tValidation Loss: 0.253194\n",
      "Validation loss decreased (0.253225 --> 0.253194).  Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 1.029018 \tValidation Loss: 0.253065\n",
      "Validation loss decreased (0.253194 --> 0.253065).  Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 1.027655 \tValidation Loss: 0.252985\n",
      "Validation loss decreased (0.253065 --> 0.252985).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 1.027708 \tValidation Loss: 0.253292\n",
      "Epoch: 154 \tTraining Loss: 1.028875 \tValidation Loss: 0.252889\n",
      "Validation loss decreased (0.252985 --> 0.252889).  Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 1.027817 \tValidation Loss: 0.252859\n",
      "Validation loss decreased (0.252889 --> 0.252859).  Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 1.026689 \tValidation Loss: 0.252681\n",
      "Validation loss decreased (0.252859 --> 0.252681).  Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 1.027159 \tValidation Loss: 0.252852\n",
      "Epoch: 158 \tTraining Loss: 1.025315 \tValidation Loss: 0.252450\n",
      "Validation loss decreased (0.252681 --> 0.252450).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 1.025314 \tValidation Loss: 0.252427\n",
      "Validation loss decreased (0.252450 --> 0.252427).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 1.026071 \tValidation Loss: 0.252450\n",
      "Epoch: 161 \tTraining Loss: 1.024718 \tValidation Loss: 0.252185\n",
      "Validation loss decreased (0.252427 --> 0.252185).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 1.023807 \tValidation Loss: 0.251989\n",
      "Validation loss decreased (0.252185 --> 0.251989).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 1.024861 \tValidation Loss: 0.252106\n",
      "Epoch: 164 \tTraining Loss: 1.024551 \tValidation Loss: 0.251715\n",
      "Validation loss decreased (0.251989 --> 0.251715).  Saving model ...\n",
      "Epoch: 165 \tTraining Loss: 1.023739 \tValidation Loss: 0.251815\n",
      "Epoch: 166 \tTraining Loss: 1.022346 \tValidation Loss: 0.251567\n",
      "Validation loss decreased (0.251715 --> 0.251567).  Saving model ...\n",
      "Epoch: 167 \tTraining Loss: 1.022977 \tValidation Loss: 0.251667\n",
      "Epoch: 168 \tTraining Loss: 1.021896 \tValidation Loss: 0.251709\n",
      "Epoch: 169 \tTraining Loss: 1.021601 \tValidation Loss: 0.251130\n",
      "Validation loss decreased (0.251567 --> 0.251130).  Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 1.021310 \tValidation Loss: 0.251190\n",
      "Epoch: 171 \tTraining Loss: 1.021929 \tValidation Loss: 0.251069\n",
      "Validation loss decreased (0.251130 --> 0.251069).  Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 1.020426 \tValidation Loss: 0.251306\n",
      "Epoch: 173 \tTraining Loss: 1.019688 \tValidation Loss: 0.250760\n",
      "Validation loss decreased (0.251069 --> 0.250760).  Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 1.020956 \tValidation Loss: 0.250838\n",
      "Epoch: 175 \tTraining Loss: 1.020230 \tValidation Loss: 0.250667\n",
      "Validation loss decreased (0.250760 --> 0.250667).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 1.018737 \tValidation Loss: 0.250690\n",
      "Epoch: 177 \tTraining Loss: 1.018701 \tValidation Loss: 0.250585\n",
      "Validation loss decreased (0.250667 --> 0.250585).  Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 1.018829 \tValidation Loss: 0.250357\n",
      "Validation loss decreased (0.250585 --> 0.250357).  Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 1.018465 \tValidation Loss: 0.250210\n",
      "Validation loss decreased (0.250357 --> 0.250210).  Saving model ...\n",
      "Epoch: 180 \tTraining Loss: 1.018111 \tValidation Loss: 0.250830\n",
      "Epoch: 181 \tTraining Loss: 1.017460 \tValidation Loss: 0.250418\n",
      "Epoch: 182 \tTraining Loss: 1.018185 \tValidation Loss: 0.250212\n",
      "Epoch: 183 \tTraining Loss: 1.015590 \tValidation Loss: 0.249827\n",
      "Validation loss decreased (0.250210 --> 0.249827).  Saving model ...\n",
      "Epoch: 184 \tTraining Loss: 1.016840 \tValidation Loss: 0.250018\n",
      "Epoch: 185 \tTraining Loss: 1.016551 \tValidation Loss: 0.249811\n",
      "Validation loss decreased (0.249827 --> 0.249811).  Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 1.014876 \tValidation Loss: 0.249811\n",
      "Epoch: 187 \tTraining Loss: 1.015475 \tValidation Loss: 0.250850\n",
      "Epoch: 188 \tTraining Loss: 1.017341 \tValidation Loss: 0.249684\n",
      "Validation loss decreased (0.249811 --> 0.249684).  Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 1.015063 \tValidation Loss: 0.249750\n",
      "Epoch: 190 \tTraining Loss: 1.014374 \tValidation Loss: 0.249379\n",
      "Validation loss decreased (0.249684 --> 0.249379).  Saving model ...\n",
      "Epoch: 191 \tTraining Loss: 1.014697 \tValidation Loss: 0.249190\n",
      "Validation loss decreased (0.249379 --> 0.249190).  Saving model ...\n",
      "Epoch: 192 \tTraining Loss: 1.013961 \tValidation Loss: 0.249010\n",
      "Validation loss decreased (0.249190 --> 0.249010).  Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 1.013622 \tValidation Loss: 0.248954\n",
      "Validation loss decreased (0.249010 --> 0.248954).  Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 1.013963 \tValidation Loss: 0.249098\n",
      "Epoch: 195 \tTraining Loss: 1.012707 \tValidation Loss: 0.250922\n",
      "Epoch: 196 \tTraining Loss: 1.013255 \tValidation Loss: 0.248858\n",
      "Validation loss decreased (0.248954 --> 0.248858).  Saving model ...\n",
      "Epoch: 197 \tTraining Loss: 1.012828 \tValidation Loss: 0.248858\n",
      "Epoch: 198 \tTraining Loss: 1.011408 \tValidation Loss: 0.248687\n",
      "Validation loss decreased (0.248858 --> 0.248687).  Saving model ...\n",
      "Epoch: 199 \tTraining Loss: 1.012784 \tValidation Loss: 0.251575\n",
      "Epoch: 200 \tTraining Loss: 1.012148 \tValidation Loss: 0.248343\n",
      "Validation loss decreased (0.248687 --> 0.248343).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "#Training the FNN Model\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "valid_loss_min = numpy.Inf \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  model_fnn.train() \n",
    "  for data, target in train_loader_fnn:\n",
    "    target = target.type(torch.LongTensor) \n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer_fnn.zero_grad()\n",
    "    output = model_fnn(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer_fnn.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  model_fnn.eval() \n",
    "  for data, target in valid_loader_fnn:\n",
    "    target = target.type(torch.LongTensor) \n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model_fnn(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "  train_loss = train_loss/len(train_loader_fnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_fnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_fnn.state_dict(), 'model_fnn.pt')\n",
    "    valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model with the lowest validation loss\n",
    "model_fnn.load_state_dict(torch.load('model_fnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the FNN Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_fnn:\n",
    "    embeddings, labels = data\n",
    "    model_fnn.to(\"cpu\")\n",
    "    outputs = model_fnn(embeddings.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function word2vec_concatenation to get values for the datasets and removing NaN values\n",
    "\n",
    "X_train_fnn_10_val = X_train.apply(lambda x: word2vec_concatenation(x, model_pre_trained))\n",
    "X_train_fnn_10 = numpy.array(X_train_fnn_10_val.values.tolist())\n",
    "\n",
    "X_test_fnn_10_val = X_test.apply(lambda x: word2vec_concatenation(x, model_pre_trained))\n",
    "X_test_fnn_10 = numpy.array(X_test_fnn_10_val.values.tolist())\n",
    "\n",
    "idx_nan_train = idx_nan(X_train_fnn_10)\n",
    "if idx_nan_train != None:\n",
    "    X_train_fnn_10_pre_trained = numpy.delete(X_train_fnn_10, idx_nan_train, 0)\n",
    "    y_train_fnn_10_pre_trained = numpy.delete(y_train, idx_nan_train)\n",
    "else:\n",
    "    X_train_fnn_10_pre_trained = X_train_fnn_10\n",
    "    y_train_fnn_10_pre_trained = y_train\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_fnn_10)\n",
    "if idx_nan_test != None:\n",
    "    X_test_fnn_10_pre_trained = numpy.delete(X_test_fnn_10, idx_nan_test, 0)\n",
    "    y_test_fnn_10_pre_trained = numpy.delete(y_test, idx_nan_test)\n",
    "else:\n",
    "    X_test_fnn_10_pre_trained = X_test_fnn_10\n",
    "    y_test_fnn_10_pre_trained = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the dataset classes\n",
    "\n",
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y\n",
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training and testing dataset for FNN Model\n",
    "\n",
    "train_fnn_10 = Train(X_train_fnn_10_pre_trained, y_train_fnn_10_pre_trained-1)\n",
    "test_fnn_10 = Test(X_test_fnn_10_pre_trained, y_test_fnn_10_pre_trained-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching and Loadind data for the FNN Model\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_fnn_10)\n",
    "indices = list(range(num_train))\n",
    "numpy.random.shuffle(indices)\n",
    "split = int(numpy.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader_fnn_10 = torch.utils.data.DataLoader(train_fnn_10, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_fnn_10 = torch.utils.data.DataLoader(train_fnn_10, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_fnn_10 = torch.utils.data.DataLoader(test_fnn_10, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerMLP(\n",
      "  (linear1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Defining the MLP Architecture and loading the model\n",
    "\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "  def __init__(self, D_in, H1, H2, D_out):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H1)\n",
    "    self.linear2 = nn.Linear(H1, H2)\n",
    "    self.linear3 = nn.Linear(H2, D_out)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1_relu = F.relu(self.linear1(x))\n",
    "    h1_drop = self.dropout(h1_relu)\n",
    "    h2_relu = F.relu(self.linear2(h1_drop))\n",
    "    h2_drop = self.dropout(h2_relu)\n",
    "    h2_output = self.linear3(h2_drop)\n",
    "\n",
    "    return h2_output\n",
    "    \n",
    "model_fnn_10 = ThreeLayerMLP(3000, 50, 10, 5)\n",
    "model_fnn_10.to(device)\n",
    "print(model_fnn_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the parameters\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_fnn_10 = torch.optim.SGD(model_fnn_10.parameters(), lr=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.295137 \tValidation Loss: 0.322230\n",
      "Validation loss decreased (inf --> 0.322230).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.286968 \tValidation Loss: 0.321317\n",
      "Validation loss decreased (0.322230 --> 0.321317).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.283529 \tValidation Loss: 0.320560\n",
      "Validation loss decreased (0.321317 --> 0.320560).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.279664 \tValidation Loss: 0.319484\n",
      "Validation loss decreased (0.320560 --> 0.319484).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.274466 \tValidation Loss: 0.317722\n",
      "Validation loss decreased (0.319484 --> 0.317722).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.265770 \tValidation Loss: 0.315138\n",
      "Validation loss decreased (0.317722 --> 0.315138).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.253480 \tValidation Loss: 0.311501\n",
      "Validation loss decreased (0.315138 --> 0.311501).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.235391 \tValidation Loss: 0.306813\n",
      "Validation loss decreased (0.311501 --> 0.306813).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.213537 \tValidation Loss: 0.301579\n",
      "Validation loss decreased (0.306813 --> 0.301579).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.193631 \tValidation Loss: 0.297076\n",
      "Validation loss decreased (0.301579 --> 0.297076).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.177077 \tValidation Loss: 0.293666\n",
      "Validation loss decreased (0.297076 --> 0.293666).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.163586 \tValidation Loss: 0.290465\n",
      "Validation loss decreased (0.293666 --> 0.290465).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.151044 \tValidation Loss: 0.288782\n",
      "Validation loss decreased (0.290465 --> 0.288782).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.143310 \tValidation Loss: 0.286971\n",
      "Validation loss decreased (0.288782 --> 0.286971).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.133466 \tValidation Loss: 0.285470\n",
      "Validation loss decreased (0.286971 --> 0.285470).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.126728 \tValidation Loss: 0.283794\n",
      "Validation loss decreased (0.285470 --> 0.283794).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.120858 \tValidation Loss: 0.282992\n",
      "Validation loss decreased (0.283794 --> 0.282992).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.116565 \tValidation Loss: 0.281880\n",
      "Validation loss decreased (0.282992 --> 0.281880).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.111107 \tValidation Loss: 0.281172\n",
      "Validation loss decreased (0.281880 --> 0.281172).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.106571 \tValidation Loss: 0.279906\n",
      "Validation loss decreased (0.281172 --> 0.279906).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.102268 \tValidation Loss: 0.279150\n",
      "Validation loss decreased (0.279906 --> 0.279150).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.098702 \tValidation Loss: 0.279225\n",
      "Epoch: 23 \tTraining Loss: 1.095226 \tValidation Loss: 0.279066\n",
      "Validation loss decreased (0.279150 --> 0.279066).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.089715 \tValidation Loss: 0.278043\n",
      "Validation loss decreased (0.279066 --> 0.278043).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.088452 \tValidation Loss: 0.277674\n",
      "Validation loss decreased (0.278043 --> 0.277674).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.084651 \tValidation Loss: 0.277021\n",
      "Validation loss decreased (0.277674 --> 0.277021).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.082192 \tValidation Loss: 0.277523\n",
      "Epoch: 28 \tTraining Loss: 1.080341 \tValidation Loss: 0.276469\n",
      "Validation loss decreased (0.277021 --> 0.276469).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.077474 \tValidation Loss: 0.276496\n",
      "Epoch: 30 \tTraining Loss: 1.074551 \tValidation Loss: 0.276990\n",
      "Epoch: 31 \tTraining Loss: 1.072165 \tValidation Loss: 0.276357\n",
      "Validation loss decreased (0.276469 --> 0.276357).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.068821 \tValidation Loss: 0.276667\n",
      "Epoch: 33 \tTraining Loss: 1.064908 \tValidation Loss: 0.275542\n",
      "Validation loss decreased (0.276357 --> 0.275542).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.064037 \tValidation Loss: 0.276471\n",
      "Epoch: 35 \tTraining Loss: 1.063217 \tValidation Loss: 0.275591\n",
      "Epoch: 36 \tTraining Loss: 1.059632 \tValidation Loss: 0.275071\n",
      "Validation loss decreased (0.275542 --> 0.275071).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.056627 \tValidation Loss: 0.275647\n",
      "Epoch: 38 \tTraining Loss: 1.054723 \tValidation Loss: 0.276260\n",
      "Epoch: 39 \tTraining Loss: 1.050749 \tValidation Loss: 0.274733\n",
      "Validation loss decreased (0.275071 --> 0.274733).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.051519 \tValidation Loss: 0.276421\n",
      "Epoch: 41 \tTraining Loss: 1.048928 \tValidation Loss: 0.276144\n",
      "Epoch: 42 \tTraining Loss: 1.045453 \tValidation Loss: 0.276082\n",
      "Epoch: 43 \tTraining Loss: 1.040823 \tValidation Loss: 0.275276\n",
      "Epoch: 44 \tTraining Loss: 1.041881 \tValidation Loss: 0.275641\n",
      "Epoch: 45 \tTraining Loss: 1.038537 \tValidation Loss: 0.275042\n",
      "Epoch: 46 \tTraining Loss: 1.034443 \tValidation Loss: 0.275470\n",
      "Epoch: 47 \tTraining Loss: 1.032377 \tValidation Loss: 0.275898\n",
      "Epoch: 48 \tTraining Loss: 1.030398 \tValidation Loss: 0.275500\n",
      "Epoch: 49 \tTraining Loss: 1.027574 \tValidation Loss: 0.275145\n",
      "Epoch: 50 \tTraining Loss: 1.024545 \tValidation Loss: 0.275796\n",
      "Epoch: 51 \tTraining Loss: 1.022997 \tValidation Loss: 0.276284\n",
      "Epoch: 52 \tTraining Loss: 1.020192 \tValidation Loss: 0.275911\n",
      "Epoch: 53 \tTraining Loss: 1.019045 \tValidation Loss: 0.275478\n",
      "Epoch: 54 \tTraining Loss: 1.016831 \tValidation Loss: 0.275972\n",
      "Epoch: 55 \tTraining Loss: 1.011719 \tValidation Loss: 0.276654\n",
      "Epoch: 56 \tTraining Loss: 1.010044 \tValidation Loss: 0.275983\n",
      "Epoch: 57 \tTraining Loss: 1.007120 \tValidation Loss: 0.277341\n",
      "Epoch: 58 \tTraining Loss: 1.002311 \tValidation Loss: 0.277420\n",
      "Epoch: 59 \tTraining Loss: 1.002609 \tValidation Loss: 0.276538\n",
      "Epoch: 60 \tTraining Loss: 0.998746 \tValidation Loss: 0.277175\n",
      "Epoch: 61 \tTraining Loss: 0.995888 \tValidation Loss: 0.278051\n",
      "Epoch: 62 \tTraining Loss: 0.995028 \tValidation Loss: 0.277694\n",
      "Epoch: 63 \tTraining Loss: 0.990337 \tValidation Loss: 0.278564\n",
      "Epoch: 64 \tTraining Loss: 0.987215 \tValidation Loss: 0.277561\n",
      "Epoch: 65 \tTraining Loss: 0.985196 \tValidation Loss: 0.278001\n",
      "Epoch: 66 \tTraining Loss: 0.981879 \tValidation Loss: 0.277920\n",
      "Epoch: 67 \tTraining Loss: 0.976632 \tValidation Loss: 0.278517\n",
      "Epoch: 68 \tTraining Loss: 0.976468 \tValidation Loss: 0.279262\n",
      "Epoch: 69 \tTraining Loss: 0.971145 \tValidation Loss: 0.278868\n",
      "Epoch: 70 \tTraining Loss: 0.967539 \tValidation Loss: 0.280472\n",
      "Epoch: 71 \tTraining Loss: 0.965325 \tValidation Loss: 0.280052\n",
      "Epoch: 72 \tTraining Loss: 0.963161 \tValidation Loss: 0.280536\n",
      "Epoch: 73 \tTraining Loss: 0.959874 \tValidation Loss: 0.280600\n",
      "Epoch: 74 \tTraining Loss: 0.956861 \tValidation Loss: 0.281217\n",
      "Epoch: 75 \tTraining Loss: 0.952541 \tValidation Loss: 0.280303\n",
      "Epoch: 76 \tTraining Loss: 0.947652 \tValidation Loss: 0.281610\n",
      "Epoch: 77 \tTraining Loss: 0.943927 \tValidation Loss: 0.282501\n",
      "Epoch: 78 \tTraining Loss: 0.941755 \tValidation Loss: 0.282835\n",
      "Epoch: 79 \tTraining Loss: 0.939031 \tValidation Loss: 0.284119\n",
      "Epoch: 80 \tTraining Loss: 0.933392 \tValidation Loss: 0.283913\n",
      "Epoch: 81 \tTraining Loss: 0.931355 \tValidation Loss: 0.283961\n",
      "Epoch: 82 \tTraining Loss: 0.929371 \tValidation Loss: 0.284391\n",
      "Epoch: 83 \tTraining Loss: 0.924566 \tValidation Loss: 0.284679\n",
      "Epoch: 84 \tTraining Loss: 0.920970 \tValidation Loss: 0.285367\n",
      "Epoch: 85 \tTraining Loss: 0.920138 \tValidation Loss: 0.285536\n",
      "Epoch: 86 \tTraining Loss: 0.911296 \tValidation Loss: 0.287617\n",
      "Epoch: 87 \tTraining Loss: 0.911369 \tValidation Loss: 0.287840\n",
      "Epoch: 88 \tTraining Loss: 0.905614 \tValidation Loss: 0.288389\n",
      "Epoch: 89 \tTraining Loss: 0.903814 \tValidation Loss: 0.287979\n",
      "Epoch: 90 \tTraining Loss: 0.899313 \tValidation Loss: 0.288677\n",
      "Epoch: 91 \tTraining Loss: 0.895312 \tValidation Loss: 0.288851\n",
      "Epoch: 92 \tTraining Loss: 0.890001 \tValidation Loss: 0.291166\n",
      "Epoch: 93 \tTraining Loss: 0.889593 \tValidation Loss: 0.291130\n",
      "Epoch: 94 \tTraining Loss: 0.885340 \tValidation Loss: 0.290595\n",
      "Epoch: 95 \tTraining Loss: 0.882744 \tValidation Loss: 0.292081\n",
      "Epoch: 96 \tTraining Loss: 0.878410 \tValidation Loss: 0.294678\n",
      "Epoch: 97 \tTraining Loss: 0.874638 \tValidation Loss: 0.297024\n",
      "Epoch: 98 \tTraining Loss: 0.870819 \tValidation Loss: 0.294689\n",
      "Epoch: 99 \tTraining Loss: 0.866128 \tValidation Loss: 0.296286\n",
      "Epoch: 100 \tTraining Loss: 0.863183 \tValidation Loss: 0.297806\n"
     ]
    }
   ],
   "source": [
    "#Training the FNN Model\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "valid_loss_min = numpy.Inf \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  model_fnn_10.train() \n",
    "  for data, target in train_loader_fnn_10:\n",
    "    target = target.type(torch.LongTensor)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer_fnn_10.zero_grad()\n",
    "    output = model_fnn_10(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer_fnn_10.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  for data, target in valid_loader_fnn_10:\n",
    "    target = target.type(torch.LongTensor)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model_fnn_10(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "  train_loss = train_loss/len(train_loader_fnn_10.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_fnn_10.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_fnn_10.state_dict(), 'model_fnn_10.pt')\n",
    "    valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model with the lowest validation loss\n",
    "model_fnn_10.load_state_dict(torch.load('model_fnn_10.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the FNN Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_fnn_10:\n",
    "    embeddings, labels = data\n",
    "    model_fnn_10.to(\"cpu\")\n",
    "    outputs = model_fnn_10(embeddings.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies of FNN is better as compared to the simple models. This is so because of the learning algorithms for FNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating values for training and testing dataset using word2vec_sequence and removing NaN values\n",
    "\n",
    "X_train_rnn_val = X_train.apply(lambda x: word2vec_sequence(x, model_pre_trained))\n",
    "X_train_rnn = numpy.array(X_train_rnn_val.values.tolist())\n",
    "\n",
    "X_test_rnn_val = X_test.apply(lambda x: word2vec_sequence(x, model_pre_trained))\n",
    "X_test_rnn = numpy.array(X_test_rnn_val.values.tolist())\n",
    "\n",
    "idx_nan_train = idx_nan(X_train_rnn)\n",
    "if idx_nan_train != None:\n",
    "  X_train_rnn_pre_trained = numpy.delete(X_train_rnn, idx_nan_train, 0)\n",
    "  y_train_rnn_pre_trained = numpy.delete(y_train, idx_nan_train)\n",
    "else:\n",
    "  X_train_rnn_pre_trained = X_train_rnn\n",
    "  y_train_rnn_pre_trained = y_train\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_rnn)\n",
    "if idx_nan_test != None:\n",
    "  X_test_rnn_pre_trained = numpy.delete(X_test_rnn, idx_nan_test, 0)\n",
    "  y_test_rnn_pre_trained = numpy.delete(y_test, idx_nan_test)\n",
    "else:\n",
    "  X_test_rnn_pre_trained = X_test_rnn\n",
    "  y_test_rnn_pre_trained = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training and testing dataset\n",
    "\n",
    "train_rnn_pre_trained = Train(X_train_rnn_pre_trained, y_train_rnn_pre_trained - 1)\n",
    "test_rnn_pre_trained = Test(X_test_rnn_pre_trained, y_test_rnn_pre_trained - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching and Loading data for the RNN Model\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_rnn_pre_trained)\n",
    "indices = list(range(num_train))\n",
    "numpy.random.shuffle(indices)\n",
    "split = int(numpy.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_rnn_pre_trained, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_rnn = torch.utils.data.DataLoader(train_rnn_pre_trained, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_rnn_pre_trained, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the RNN Architecture\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True,\n",
    "                     nonlinearity='relu')\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    out, hn = self.rnn(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initializing the RNN Model\n",
    "\n",
    "model_rnn = RNNModel(300, 20, 1, 5)\n",
    "model_rnn.cuda()\n",
    "print(model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Parameters for the RNN Model\n",
    "\n",
    "optimizer_rnn = torch.optim.SGD(model_rnn.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.288773 \tValidation Loss: 0.321832\n",
      "Validation loss decreased (inf --> 0.321832).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.287371 \tValidation Loss: 0.321800\n",
      "Validation loss decreased (0.321832 --> 0.321800).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.287258 \tValidation Loss: 0.321784\n",
      "Validation loss decreased (0.321800 --> 0.321784).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.287198 \tValidation Loss: 0.321772\n",
      "Validation loss decreased (0.321784 --> 0.321772).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.287117 \tValidation Loss: 0.321756\n",
      "Validation loss decreased (0.321772 --> 0.321756).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.287017 \tValidation Loss: 0.321729\n",
      "Validation loss decreased (0.321756 --> 0.321729).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.286842 \tValidation Loss: 0.321672\n",
      "Validation loss decreased (0.321729 --> 0.321672).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.286565 \tValidation Loss: 0.321572\n",
      "Validation loss decreased (0.321672 --> 0.321572).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.285915 \tValidation Loss: 0.321338\n",
      "Validation loss decreased (0.321572 --> 0.321338).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.284107 \tValidation Loss: 0.320666\n",
      "Validation loss decreased (0.321338 --> 0.320666).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.277922 \tValidation Loss: 0.318604\n",
      "Validation loss decreased (0.320666 --> 0.318604).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.261571 \tValidation Loss: 0.312920\n",
      "Validation loss decreased (0.318604 --> 0.312920).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.233719 \tValidation Loss: 0.305659\n",
      "Validation loss decreased (0.312920 --> 0.305659).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.207350 \tValidation Loss: 0.301147\n",
      "Validation loss decreased (0.305659 --> 0.301147).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.189772 \tValidation Loss: 0.297260\n",
      "Validation loss decreased (0.301147 --> 0.297260).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.176041 \tValidation Loss: 0.291017\n",
      "Validation loss decreased (0.297260 --> 0.291017).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.165438 \tValidation Loss: 0.289707\n",
      "Validation loss decreased (0.291017 --> 0.289707).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.156141 \tValidation Loss: 0.287302\n",
      "Validation loss decreased (0.289707 --> 0.287302).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.149710 \tValidation Loss: 0.285005\n",
      "Validation loss decreased (0.287302 --> 0.285005).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.143312 \tValidation Loss: 0.284123\n",
      "Validation loss decreased (0.285005 --> 0.284123).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.138462 \tValidation Loss: 0.289063\n",
      "Epoch: 22 \tTraining Loss: 1.132999 \tValidation Loss: 0.286879\n",
      "Epoch: 23 \tTraining Loss: 1.126655 \tValidation Loss: 0.280145\n",
      "Validation loss decreased (0.284123 --> 0.280145).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.122013 \tValidation Loss: 0.279019\n",
      "Validation loss decreased (0.280145 --> 0.279019).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.118963 \tValidation Loss: 0.278298\n",
      "Validation loss decreased (0.279019 --> 0.278298).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.114233 \tValidation Loss: 0.277716\n",
      "Validation loss decreased (0.278298 --> 0.277716).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.111809 \tValidation Loss: 0.292718\n",
      "Epoch: 28 \tTraining Loss: 1.108435 \tValidation Loss: 0.277686\n",
      "Validation loss decreased (0.277716 --> 0.277686).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.105988 \tValidation Loss: 0.275350\n",
      "Validation loss decreased (0.277686 --> 0.275350).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.102635 \tValidation Loss: 0.274893\n",
      "Validation loss decreased (0.275350 --> 0.274893).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.100194 \tValidation Loss: 0.275621\n",
      "Epoch: 32 \tTraining Loss: 1.097205 \tValidation Loss: 0.275561\n",
      "Epoch: 33 \tTraining Loss: 1.094421 \tValidation Loss: 0.276307\n",
      "Epoch: 34 \tTraining Loss: 1.092323 \tValidation Loss: 0.272968\n",
      "Validation loss decreased (0.274893 --> 0.272968).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.091741 \tValidation Loss: 0.272599\n",
      "Validation loss decreased (0.272968 --> 0.272599).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.087805 \tValidation Loss: 0.275841\n",
      "Epoch: 37 \tTraining Loss: 1.087729 \tValidation Loss: 0.272518\n",
      "Validation loss decreased (0.272599 --> 0.272518).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.085356 \tValidation Loss: 0.272449\n",
      "Validation loss decreased (0.272518 --> 0.272449).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.084997 \tValidation Loss: 0.282329\n",
      "Epoch: 40 \tTraining Loss: 1.082628 \tValidation Loss: 0.272718\n",
      "Epoch: 41 \tTraining Loss: 1.080480 \tValidation Loss: 0.272589\n",
      "Epoch: 42 \tTraining Loss: 1.077804 \tValidation Loss: 0.270887\n",
      "Validation loss decreased (0.272449 --> 0.270887).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.077900 \tValidation Loss: 0.269615\n",
      "Validation loss decreased (0.270887 --> 0.269615).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.075386 \tValidation Loss: 0.270312\n",
      "Epoch: 45 \tTraining Loss: 1.073838 \tValidation Loss: 0.273493\n",
      "Epoch: 46 \tTraining Loss: 1.073195 \tValidation Loss: 0.280337\n",
      "Epoch: 47 \tTraining Loss: 1.072039 \tValidation Loss: 0.268801\n",
      "Validation loss decreased (0.269615 --> 0.268801).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.070410 \tValidation Loss: 0.274767\n",
      "Epoch: 49 \tTraining Loss: 1.069465 \tValidation Loss: 0.270562\n",
      "Epoch: 50 \tTraining Loss: 1.067753 \tValidation Loss: 0.268154\n",
      "Validation loss decreased (0.268801 --> 0.268154).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.066968 \tValidation Loss: 0.267688\n",
      "Validation loss decreased (0.268154 --> 0.267688).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 1.066406 \tValidation Loss: 0.267619\n",
      "Validation loss decreased (0.267688 --> 0.267619).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.066102 \tValidation Loss: 0.268239\n",
      "Epoch: 54 \tTraining Loss: 1.064411 \tValidation Loss: 0.267397\n",
      "Validation loss decreased (0.267619 --> 0.267397).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 1.062483 \tValidation Loss: 0.267407\n",
      "Epoch: 56 \tTraining Loss: 1.061677 \tValidation Loss: 0.268526\n",
      "Epoch: 57 \tTraining Loss: 1.061080 \tValidation Loss: 0.266371\n",
      "Validation loss decreased (0.267397 --> 0.266371).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.059307 \tValidation Loss: 0.266190\n",
      "Validation loss decreased (0.266371 --> 0.266190).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.059342 \tValidation Loss: 0.265792\n",
      "Validation loss decreased (0.266190 --> 0.265792).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 1.058871 \tValidation Loss: 0.266530\n",
      "Epoch: 61 \tTraining Loss: 1.057224 \tValidation Loss: 0.266343\n",
      "Epoch: 62 \tTraining Loss: 1.056046 \tValidation Loss: 0.265730\n",
      "Validation loss decreased (0.265792 --> 0.265730).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 1.055976 \tValidation Loss: 0.265659\n",
      "Validation loss decreased (0.265730 --> 0.265659).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 1.054654 \tValidation Loss: 0.265537\n",
      "Validation loss decreased (0.265659 --> 0.265537).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 1.053453 \tValidation Loss: 0.265679\n",
      "Epoch: 66 \tTraining Loss: 1.053566 \tValidation Loss: 0.264708\n",
      "Validation loss decreased (0.265537 --> 0.264708).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 1.053632 \tValidation Loss: 0.268248\n",
      "Epoch: 68 \tTraining Loss: 1.051923 \tValidation Loss: 0.265267\n",
      "Epoch: 69 \tTraining Loss: 1.050656 \tValidation Loss: 0.264456\n",
      "Validation loss decreased (0.264708 --> 0.264456).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 1.050679 \tValidation Loss: 0.264090\n",
      "Validation loss decreased (0.264456 --> 0.264090).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 1.049473 \tValidation Loss: 0.267326\n",
      "Epoch: 72 \tTraining Loss: 1.049297 \tValidation Loss: 0.264918\n",
      "Epoch: 73 \tTraining Loss: 1.049807 \tValidation Loss: 0.264090\n",
      "Validation loss decreased (0.264090 --> 0.264090).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 1.048414 \tValidation Loss: 0.265202\n",
      "Epoch: 75 \tTraining Loss: 1.045929 \tValidation Loss: 0.266319\n",
      "Epoch: 76 \tTraining Loss: 1.046512 \tValidation Loss: 0.265074\n",
      "Epoch: 77 \tTraining Loss: 1.045290 \tValidation Loss: 0.266180\n",
      "Epoch: 78 \tTraining Loss: 1.044972 \tValidation Loss: 0.264630\n",
      "Epoch: 79 \tTraining Loss: 1.043785 \tValidation Loss: 0.262814\n",
      "Validation loss decreased (0.264090 --> 0.262814).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 1.044321 \tValidation Loss: 0.265150\n",
      "Epoch: 81 \tTraining Loss: 1.042866 \tValidation Loss: 0.263448\n",
      "Epoch: 82 \tTraining Loss: 1.042237 \tValidation Loss: 0.263088\n",
      "Epoch: 83 \tTraining Loss: 1.041404 \tValidation Loss: 0.262321\n",
      "Validation loss decreased (0.262814 --> 0.262321).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 1.041618 \tValidation Loss: 0.270880\n",
      "Epoch: 85 \tTraining Loss: 1.039834 \tValidation Loss: 0.268799\n",
      "Epoch: 86 \tTraining Loss: 1.040224 \tValidation Loss: 0.272389\n",
      "Epoch: 87 \tTraining Loss: 1.038508 \tValidation Loss: 0.265871\n",
      "Epoch: 88 \tTraining Loss: 1.037194 \tValidation Loss: 0.263328\n",
      "Epoch: 89 \tTraining Loss: 1.037220 \tValidation Loss: 0.262663\n",
      "Epoch: 90 \tTraining Loss: 1.035869 \tValidation Loss: 0.263206\n",
      "Epoch: 91 \tTraining Loss: 1.035546 \tValidation Loss: 0.263199\n",
      "Epoch: 92 \tTraining Loss: 1.035950 \tValidation Loss: 0.272951\n",
      "Epoch: 93 \tTraining Loss: 1.034743 \tValidation Loss: 0.263980\n",
      "Epoch: 94 \tTraining Loss: 1.034937 \tValidation Loss: 0.265426\n",
      "Epoch: 95 \tTraining Loss: 1.032893 \tValidation Loss: 0.261793\n",
      "Validation loss decreased (0.262321 --> 0.261793).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 1.033015 \tValidation Loss: 0.263663\n",
      "Epoch: 97 \tTraining Loss: 1.031175 \tValidation Loss: 0.261170\n",
      "Validation loss decreased (0.261793 --> 0.261170).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 1.033072 \tValidation Loss: 0.260932\n",
      "Validation loss decreased (0.261170 --> 0.260932).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 1.031578 \tValidation Loss: 0.263187\n",
      "Epoch: 100 \tTraining Loss: 1.030535 \tValidation Loss: 0.264486\n",
      "Epoch: 101 \tTraining Loss: 1.030168 \tValidation Loss: 0.260884\n",
      "Validation loss decreased (0.260932 --> 0.260884).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 1.030366 \tValidation Loss: 0.260868\n",
      "Validation loss decreased (0.260884 --> 0.260868).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 1.029517 \tValidation Loss: 0.260639\n",
      "Validation loss decreased (0.260868 --> 0.260639).  Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 1.029215 \tValidation Loss: 0.265650\n",
      "Epoch: 105 \tTraining Loss: 1.027825 \tValidation Loss: 0.269183\n",
      "Epoch: 106 \tTraining Loss: 1.026934 \tValidation Loss: 0.261308\n",
      "Epoch: 107 \tTraining Loss: 1.025686 \tValidation Loss: 0.260242\n",
      "Validation loss decreased (0.260639 --> 0.260242).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 1.024497 \tValidation Loss: 0.260276\n",
      "Epoch: 109 \tTraining Loss: 1.025550 \tValidation Loss: 0.262621\n",
      "Epoch: 110 \tTraining Loss: 1.023573 \tValidation Loss: 0.263076\n",
      "Epoch: 111 \tTraining Loss: 1.021297 \tValidation Loss: 0.259718\n",
      "Validation loss decreased (0.260242 --> 0.259718).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 1.021368 \tValidation Loss: 0.263795\n",
      "Epoch: 113 \tTraining Loss: 1.020470 \tValidation Loss: 0.265485\n",
      "Epoch: 114 \tTraining Loss: 1.019700 \tValidation Loss: 0.260578\n",
      "Epoch: 115 \tTraining Loss: 1.020412 \tValidation Loss: 0.258271\n",
      "Validation loss decreased (0.259718 --> 0.258271).  Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 1.019674 \tValidation Loss: 0.268079\n",
      "Epoch: 117 \tTraining Loss: 1.017559 \tValidation Loss: 0.258231\n",
      "Validation loss decreased (0.258271 --> 0.258231).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 1.017779 \tValidation Loss: 0.258597\n",
      "Epoch: 119 \tTraining Loss: 1.016180 \tValidation Loss: 0.257798\n",
      "Validation loss decreased (0.258231 --> 0.257798).  Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 1.015869 \tValidation Loss: 0.260703\n",
      "Epoch: 121 \tTraining Loss: 1.016346 \tValidation Loss: 0.261075\n",
      "Epoch: 122 \tTraining Loss: 1.014715 \tValidation Loss: 0.258507\n",
      "Epoch: 123 \tTraining Loss: 1.014412 \tValidation Loss: 0.258807\n",
      "Epoch: 124 \tTraining Loss: 1.012746 \tValidation Loss: 0.257342\n",
      "Validation loss decreased (0.257798 --> 0.257342).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 1.013489 \tValidation Loss: 0.257550\n",
      "Epoch: 126 \tTraining Loss: 1.011700 \tValidation Loss: 0.259124\n",
      "Epoch: 127 \tTraining Loss: 1.011417 \tValidation Loss: 0.256808\n",
      "Validation loss decreased (0.257342 --> 0.256808).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 1.009083 \tValidation Loss: 0.257376\n",
      "Epoch: 129 \tTraining Loss: 1.011801 \tValidation Loss: 0.257650\n",
      "Epoch: 130 \tTraining Loss: 1.009992 \tValidation Loss: 0.257588\n",
      "Epoch: 131 \tTraining Loss: 1.011256 \tValidation Loss: 0.257074\n",
      "Epoch: 132 \tTraining Loss: 1.008656 \tValidation Loss: 0.258499\n",
      "Epoch: 133 \tTraining Loss: 1.009185 \tValidation Loss: 0.261904\n",
      "Epoch: 134 \tTraining Loss: 1.006438 \tValidation Loss: 0.256185\n",
      "Validation loss decreased (0.256808 --> 0.256185).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 1.006342 \tValidation Loss: 0.258568\n",
      "Epoch: 136 \tTraining Loss: 1.006765 \tValidation Loss: 0.261342\n",
      "Epoch: 137 \tTraining Loss: 1.006395 \tValidation Loss: 0.257296\n",
      "Epoch: 138 \tTraining Loss: 1.006125 \tValidation Loss: 0.257581\n",
      "Epoch: 139 \tTraining Loss: 1.004661 \tValidation Loss: 0.256314\n",
      "Epoch: 140 \tTraining Loss: 1.003734 \tValidation Loss: 0.258734\n",
      "Epoch: 141 \tTraining Loss: 1.003791 \tValidation Loss: 0.255117\n",
      "Validation loss decreased (0.256185 --> 0.255117).  Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 1.001706 \tValidation Loss: 0.268146\n",
      "Epoch: 143 \tTraining Loss: 1.003268 \tValidation Loss: 0.259076\n",
      "Epoch: 144 \tTraining Loss: 1.001936 \tValidation Loss: 0.255846\n",
      "Epoch: 145 \tTraining Loss: 1.002212 \tValidation Loss: 0.255429\n",
      "Epoch: 146 \tTraining Loss: 1.001567 \tValidation Loss: 0.255720\n",
      "Epoch: 147 \tTraining Loss: 1.000736 \tValidation Loss: 0.255729\n",
      "Epoch: 148 \tTraining Loss: 0.999267 \tValidation Loss: 0.258985\n",
      "Epoch: 149 \tTraining Loss: 1.000171 \tValidation Loss: 0.254113\n",
      "Validation loss decreased (0.255117 --> 0.254113).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 0.999207 \tValidation Loss: 0.260628\n",
      "Epoch: 151 \tTraining Loss: 0.998549 \tValidation Loss: 0.258780\n",
      "Epoch: 152 \tTraining Loss: 0.997039 \tValidation Loss: 0.254098\n",
      "Validation loss decreased (0.254113 --> 0.254098).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 0.997974 \tValidation Loss: 0.261180\n",
      "Epoch: 154 \tTraining Loss: 0.996727 \tValidation Loss: 0.255916\n",
      "Epoch: 155 \tTraining Loss: 0.997176 \tValidation Loss: 0.255853\n",
      "Epoch: 156 \tTraining Loss: 0.994860 \tValidation Loss: 0.255580\n",
      "Epoch: 157 \tTraining Loss: 0.995866 \tValidation Loss: 0.256825\n",
      "Epoch: 158 \tTraining Loss: 0.995977 \tValidation Loss: 0.254933\n",
      "Epoch: 159 \tTraining Loss: 0.993400 \tValidation Loss: 0.253701\n",
      "Validation loss decreased (0.254098 --> 0.253701).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 0.993850 \tValidation Loss: 0.254261\n",
      "Epoch: 161 \tTraining Loss: 0.994861 \tValidation Loss: 0.253602\n",
      "Validation loss decreased (0.253701 --> 0.253602).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 0.993480 \tValidation Loss: 0.253153\n",
      "Validation loss decreased (0.253602 --> 0.253153).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 0.994067 \tValidation Loss: 0.255732\n",
      "Epoch: 164 \tTraining Loss: 0.991735 \tValidation Loss: 0.258306\n",
      "Epoch: 165 \tTraining Loss: 0.992099 \tValidation Loss: 0.253698\n",
      "Epoch: 166 \tTraining Loss: 0.991565 \tValidation Loss: 0.261196\n",
      "Epoch: 167 \tTraining Loss: 0.990731 \tValidation Loss: 0.255010\n",
      "Epoch: 168 \tTraining Loss: 0.991934 \tValidation Loss: 0.255146\n",
      "Epoch: 169 \tTraining Loss: 0.989937 \tValidation Loss: 0.256525\n",
      "Epoch: 170 \tTraining Loss: 0.988330 \tValidation Loss: 0.256138\n",
      "Epoch: 171 \tTraining Loss: 0.989031 \tValidation Loss: 0.253081\n",
      "Validation loss decreased (0.253153 --> 0.253081).  Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 0.987857 \tValidation Loss: 0.257240\n",
      "Epoch: 173 \tTraining Loss: 0.988797 \tValidation Loss: 0.255205\n",
      "Epoch: 174 \tTraining Loss: 0.988692 \tValidation Loss: 0.254739\n",
      "Epoch: 175 \tTraining Loss: 0.988124 \tValidation Loss: 0.253078\n",
      "Validation loss decreased (0.253081 --> 0.253078).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 0.987158 \tValidation Loss: 0.256741\n",
      "Epoch: 177 \tTraining Loss: 0.986776 \tValidation Loss: 0.258848\n",
      "Epoch: 178 \tTraining Loss: 0.988882 \tValidation Loss: 0.254224\n",
      "Epoch: 179 \tTraining Loss: 0.985729 \tValidation Loss: 0.266052\n",
      "Epoch: 180 \tTraining Loss: 0.984563 \tValidation Loss: 0.253326\n",
      "Epoch: 181 \tTraining Loss: 0.983555 \tValidation Loss: 0.254617\n",
      "Epoch: 182 \tTraining Loss: 0.984714 \tValidation Loss: 0.252793\n",
      "Validation loss decreased (0.253078 --> 0.252793).  Saving model ...\n",
      "Epoch: 183 \tTraining Loss: 0.983255 \tValidation Loss: 0.263109\n",
      "Epoch: 184 \tTraining Loss: 0.983727 \tValidation Loss: 0.253366\n",
      "Epoch: 185 \tTraining Loss: 0.983919 \tValidation Loss: 0.254571\n",
      "Epoch: 186 \tTraining Loss: 0.984429 \tValidation Loss: 0.258022\n",
      "Epoch: 187 \tTraining Loss: 0.982252 \tValidation Loss: 0.255404\n",
      "Epoch: 188 \tTraining Loss: 0.981567 \tValidation Loss: 0.252993\n",
      "Epoch: 189 \tTraining Loss: 0.981609 \tValidation Loss: 0.253944\n",
      "Epoch: 190 \tTraining Loss: 0.982566 \tValidation Loss: 0.254297\n",
      "Epoch: 191 \tTraining Loss: 0.980731 \tValidation Loss: 0.253790\n",
      "Epoch: 192 \tTraining Loss: 0.979741 \tValidation Loss: 0.265564\n",
      "Epoch: 193 \tTraining Loss: 0.981446 \tValidation Loss: 0.252555\n",
      "Validation loss decreased (0.252793 --> 0.252555).  Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 0.979329 \tValidation Loss: 0.266101\n",
      "Epoch: 195 \tTraining Loss: 0.979631 \tValidation Loss: 0.253373\n",
      "Epoch: 196 \tTraining Loss: 0.978432 \tValidation Loss: 0.255021\n",
      "Epoch: 197 \tTraining Loss: 0.980986 \tValidation Loss: 0.254494\n",
      "Epoch: 198 \tTraining Loss: 0.976622 \tValidation Loss: 0.258437\n",
      "Epoch: 199 \tTraining Loss: 0.979253 \tValidation Loss: 0.258192\n",
      "Epoch: 200 \tTraining Loss: 0.979881 \tValidation Loss: 0.253168\n"
     ]
    }
   ],
   "source": [
    "#Training the RNN Model\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "valid_loss_min = numpy.Inf \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  model_rnn.train() \n",
    "\n",
    "  for data, target in train_loader_rnn:\n",
    "    target = target.type(torch.LongTensor)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer_rnn.zero_grad()\n",
    "    output = model_rnn(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer_rnn.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  model_rnn.eval() \n",
    "\n",
    "  for data, target in valid_loader_rnn:\n",
    "    target = target.type(torch.LongTensor)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model_rnn(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  train_loss = train_loss/len(train_loader_rnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_rnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_rnn.state_dict(), 'model_rnn.pt')\n",
    "    valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model with the lowest validation loss\n",
    "model_rnn.load_state_dict(torch.load('model_rnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RNN Model: 44 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the RNN Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_rnn:\n",
    "    embeddings, labels = data\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    model_rnn.to(device)\n",
    "    outputs = model_rnn(embeddings.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "print('Accuracy of RNN Model: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN works better when the dataset provided is large, but since we are working on smaller batches and data, the accuracy of FNN is much better as compared to RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function word2vec_sequence to get values for training and testing data, and removing any NaN values\n",
    "\n",
    "X_train_gru_val = X_train.apply(lambda x: word2vec_sequence(x, model_pre_trained))\n",
    "X_train_gru = numpy.array(X_train_gru_val.values.tolist())\n",
    "\n",
    "X_test_gru_val = X_test.apply(lambda x: word2vec_sequence(x, model_pre_trained))\n",
    "X_test_gru = numpy.array(X_test_gru_val.values.tolist())\n",
    "\n",
    "idx_nan_train = idx_nan(X_train_gru)\n",
    "if idx_nan_train != None:\n",
    "  X_train_gru_pre_trained = numpy.delete(X_train_gru, idx_nan_train, 0)\n",
    "  y_train_gru_pre_trained = numpy.delete(y_train, idx_nan_train)\n",
    "else:\n",
    "  X_train_gru_pre_trained = X_train_gru\n",
    "  y_train_gru_pre_trained = y_train\n",
    "\n",
    "idx_nan_test = idx_nan(X_test_gru)\n",
    "if idx_nan_test != None:\n",
    "  X_test_gru_pre_trained = numpy.delete(X_test_gru, idx_nan_test, 0)\n",
    "  y_test_gru_pre_trained = numpy.delete(y_test, idx_nan_test)\n",
    "else:\n",
    "  X_test_gru_pre_trained = X_test_gru\n",
    "  y_test_gru_pre_trained = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the data classes\n",
    "\n",
    "class Train(Dataset):\n",
    "  def __init__(self, xtrain, ytrain):\n",
    "    'Initialization'\n",
    "    self.data = xtrain\n",
    "    self.labels = ytrain\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y\n",
    "class Test(Dataset):\n",
    "  def __init__(self, xtest, ytest):\n",
    "    'Initialization'\n",
    "    self.data = xtest\n",
    "    self.labels = ytest\n",
    "\n",
    "  def __len__(self):\n",
    "    'Denotes the total number of samples'\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    'Generates one sample of data'\n",
    "    X = self.data[index]\n",
    "    y = self.labels[index]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training and testing dataset\n",
    "\n",
    "train_gru_pre_trained = Train(X_train_gru_pre_trained, y_train_gru_pre_trained - 1) \n",
    "test_gru_pre_trained = Test(X_test_gru_pre_trained, y_test_gru_pre_trained - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching and Loading the Data for the GRU Model\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_gru_pre_trained)\n",
    "indices = list(range(num_train))\n",
    "numpy.random.shuffle(indices)\n",
    "split = int(numpy.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader_gru = torch.utils.data.DataLoader(train_gru_pre_trained, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_gru = torch.utils.data.DataLoader(train_gru_pre_trained, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_gru = torch.utils.data.DataLoader(test_gru_pre_trained, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the GRU Architecture\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.layer_dim = layer_dim\n",
    "    self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    out, hn = self.gru(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initializing the GRU Model\n",
    "\n",
    "model_gru = GRUModel(300, 20, 1, 5)\n",
    "model_gru.cuda()\n",
    "print(model_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the GRU Parameters\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_gru = torch.optim.SGD(model_gru.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.291488 \tValidation Loss: 0.321927\n",
      "Validation loss decreased (inf --> 0.321927).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.287266 \tValidation Loss: 0.321792\n",
      "Validation loss decreased (0.321927 --> 0.321792).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.287033 \tValidation Loss: 0.321759\n",
      "Validation loss decreased (0.321792 --> 0.321759).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.286880 \tValidation Loss: 0.321720\n",
      "Validation loss decreased (0.321759 --> 0.321720).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.286723 \tValidation Loss: 0.321686\n",
      "Validation loss decreased (0.321720 --> 0.321686).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.286538 \tValidation Loss: 0.321642\n",
      "Validation loss decreased (0.321686 --> 0.321642).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.286359 \tValidation Loss: 0.321599\n",
      "Validation loss decreased (0.321642 --> 0.321599).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.286143 \tValidation Loss: 0.321549\n",
      "Validation loss decreased (0.321599 --> 0.321549).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.285912 \tValidation Loss: 0.321496\n",
      "Validation loss decreased (0.321549 --> 0.321496).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.285652 \tValidation Loss: 0.321435\n",
      "Validation loss decreased (0.321496 --> 0.321435).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.285346 \tValidation Loss: 0.321360\n",
      "Validation loss decreased (0.321435 --> 0.321360).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.285000 \tValidation Loss: 0.321277\n",
      "Validation loss decreased (0.321360 --> 0.321277).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.284605 \tValidation Loss: 0.321184\n",
      "Validation loss decreased (0.321277 --> 0.321184).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.284149 \tValidation Loss: 0.321076\n",
      "Validation loss decreased (0.321184 --> 0.321076).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.283583 \tValidation Loss: 0.320947\n",
      "Validation loss decreased (0.321076 --> 0.320947).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.282946 \tValidation Loss: 0.320780\n",
      "Validation loss decreased (0.320947 --> 0.320780).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.282155 \tValidation Loss: 0.320597\n",
      "Validation loss decreased (0.320780 --> 0.320597).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.281202 \tValidation Loss: 0.320363\n",
      "Validation loss decreased (0.320597 --> 0.320363).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.280023 \tValidation Loss: 0.320068\n",
      "Validation loss decreased (0.320363 --> 0.320068).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.278508 \tValidation Loss: 0.319715\n",
      "Validation loss decreased (0.320068 --> 0.319715).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.276534 \tValidation Loss: 0.319152\n",
      "Validation loss decreased (0.319715 --> 0.319152).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.273605 \tValidation Loss: 0.318267\n",
      "Validation loss decreased (0.319152 --> 0.318267).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.267886 \tValidation Loss: 0.316152\n",
      "Validation loss decreased (0.318267 --> 0.316152).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.248868 \tValidation Loss: 0.307565\n",
      "Validation loss decreased (0.316152 --> 0.307565).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.205749 \tValidation Loss: 0.299042\n",
      "Validation loss decreased (0.307565 --> 0.299042).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.178846 \tValidation Loss: 0.294328\n",
      "Validation loss decreased (0.299042 --> 0.294328).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.162711 \tValidation Loss: 0.291156\n",
      "Validation loss decreased (0.294328 --> 0.291156).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.151295 \tValidation Loss: 0.292083\n",
      "Epoch: 29 \tTraining Loss: 1.142704 \tValidation Loss: 0.287692\n",
      "Validation loss decreased (0.291156 --> 0.287692).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.135588 \tValidation Loss: 0.285209\n",
      "Validation loss decreased (0.287692 --> 0.285209).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.129280 \tValidation Loss: 0.283811\n",
      "Validation loss decreased (0.285209 --> 0.283811).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.124407 \tValidation Loss: 0.283357\n",
      "Validation loss decreased (0.283811 --> 0.283357).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.119741 \tValidation Loss: 0.282307\n",
      "Validation loss decreased (0.283357 --> 0.282307).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.115494 \tValidation Loss: 0.280868\n",
      "Validation loss decreased (0.282307 --> 0.280868).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.111557 \tValidation Loss: 0.279740\n",
      "Validation loss decreased (0.280868 --> 0.279740).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.107870 \tValidation Loss: 0.278897\n",
      "Validation loss decreased (0.279740 --> 0.278897).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.105272 \tValidation Loss: 0.278142\n",
      "Validation loss decreased (0.278897 --> 0.278142).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.101850 \tValidation Loss: 0.277491\n",
      "Validation loss decreased (0.278142 --> 0.277491).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.098926 \tValidation Loss: 0.276881\n",
      "Validation loss decreased (0.277491 --> 0.276881).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.096952 \tValidation Loss: 0.276776\n",
      "Validation loss decreased (0.276881 --> 0.276776).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.094226 \tValidation Loss: 0.275896\n",
      "Validation loss decreased (0.276776 --> 0.275896).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.092195 \tValidation Loss: 0.276919\n",
      "Epoch: 43 \tTraining Loss: 1.090886 \tValidation Loss: 0.274894\n",
      "Validation loss decreased (0.275896 --> 0.274894).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.088246 \tValidation Loss: 0.274454\n",
      "Validation loss decreased (0.274894 --> 0.274454).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.086785 \tValidation Loss: 0.274205\n",
      "Validation loss decreased (0.274454 --> 0.274205).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.085113 \tValidation Loss: 0.273638\n",
      "Validation loss decreased (0.274205 --> 0.273638).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.084403 \tValidation Loss: 0.274587\n",
      "Epoch: 48 \tTraining Loss: 1.082104 \tValidation Loss: 0.273040\n",
      "Validation loss decreased (0.273638 --> 0.273040).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.080870 \tValidation Loss: 0.273304\n",
      "Epoch: 50 \tTraining Loss: 1.079845 \tValidation Loss: 0.272646\n",
      "Validation loss decreased (0.273040 --> 0.272646).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.078761 \tValidation Loss: 0.273764\n",
      "Epoch: 52 \tTraining Loss: 1.077331 \tValidation Loss: 0.273683\n",
      "Epoch: 53 \tTraining Loss: 1.076453 \tValidation Loss: 0.271735\n",
      "Validation loss decreased (0.272646 --> 0.271735).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.075777 \tValidation Loss: 0.278468\n",
      "Epoch: 55 \tTraining Loss: 1.074940 \tValidation Loss: 0.272545\n",
      "Epoch: 56 \tTraining Loss: 1.073557 \tValidation Loss: 0.271785\n",
      "Epoch: 57 \tTraining Loss: 1.072794 \tValidation Loss: 0.271361\n",
      "Validation loss decreased (0.271735 --> 0.271361).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.071650 \tValidation Loss: 0.270854\n",
      "Validation loss decreased (0.271361 --> 0.270854).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.071209 \tValidation Loss: 0.270933\n",
      "Epoch: 60 \tTraining Loss: 1.069517 \tValidation Loss: 0.270623\n",
      "Validation loss decreased (0.270854 --> 0.270623).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 1.069357 \tValidation Loss: 0.270233\n",
      "Validation loss decreased (0.270623 --> 0.270233).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 1.068229 \tValidation Loss: 0.270008\n",
      "Validation loss decreased (0.270233 --> 0.270008).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 1.067265 \tValidation Loss: 0.271303\n",
      "Epoch: 64 \tTraining Loss: 1.066149 \tValidation Loss: 0.273654\n",
      "Epoch: 65 \tTraining Loss: 1.065292 \tValidation Loss: 0.269523\n",
      "Validation loss decreased (0.270008 --> 0.269523).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 1.064788 \tValidation Loss: 0.269299\n",
      "Validation loss decreased (0.269523 --> 0.269299).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 1.064082 \tValidation Loss: 0.270145\n",
      "Epoch: 68 \tTraining Loss: 1.063129 \tValidation Loss: 0.269187\n",
      "Validation loss decreased (0.269299 --> 0.269187).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 1.062392 \tValidation Loss: 0.269172\n",
      "Validation loss decreased (0.269187 --> 0.269172).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 1.061699 \tValidation Loss: 0.268569\n",
      "Validation loss decreased (0.269172 --> 0.268569).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 1.060543 \tValidation Loss: 0.270466\n",
      "Epoch: 72 \tTraining Loss: 1.059805 \tValidation Loss: 0.268855\n",
      "Epoch: 73 \tTraining Loss: 1.059043 \tValidation Loss: 0.271094\n",
      "Epoch: 74 \tTraining Loss: 1.058397 \tValidation Loss: 0.270867\n",
      "Epoch: 75 \tTraining Loss: 1.057109 \tValidation Loss: 0.267805\n",
      "Validation loss decreased (0.268569 --> 0.267805).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 1.055793 \tValidation Loss: 0.267705\n",
      "Validation loss decreased (0.267805 --> 0.267705).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 1.055263 \tValidation Loss: 0.267514\n",
      "Validation loss decreased (0.267705 --> 0.267514).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 1.054625 \tValidation Loss: 0.268356\n",
      "Epoch: 79 \tTraining Loss: 1.053554 \tValidation Loss: 0.267022\n",
      "Validation loss decreased (0.267514 --> 0.267022).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 1.052047 \tValidation Loss: 0.268043\n",
      "Epoch: 81 \tTraining Loss: 1.051196 \tValidation Loss: 0.266587\n",
      "Validation loss decreased (0.267022 --> 0.266587).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 1.050358 \tValidation Loss: 0.269039\n",
      "Epoch: 83 \tTraining Loss: 1.049279 \tValidation Loss: 0.265577\n",
      "Validation loss decreased (0.266587 --> 0.265577).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 1.046961 \tValidation Loss: 0.266695\n",
      "Epoch: 85 \tTraining Loss: 1.046243 \tValidation Loss: 0.266227\n",
      "Epoch: 86 \tTraining Loss: 1.043028 \tValidation Loss: 0.264632\n",
      "Validation loss decreased (0.265577 --> 0.264632).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 1.041465 \tValidation Loss: 0.264764\n",
      "Epoch: 88 \tTraining Loss: 1.039163 \tValidation Loss: 0.263126\n",
      "Validation loss decreased (0.264632 --> 0.263126).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 1.035589 \tValidation Loss: 0.262637\n",
      "Validation loss decreased (0.263126 --> 0.262637).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 1.032857 \tValidation Loss: 0.262117\n",
      "Validation loss decreased (0.262637 --> 0.262117).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 1.030252 \tValidation Loss: 0.261522\n",
      "Validation loss decreased (0.262117 --> 0.261522).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 1.027762 \tValidation Loss: 0.262784\n",
      "Epoch: 93 \tTraining Loss: 1.026389 \tValidation Loss: 0.260882\n",
      "Validation loss decreased (0.261522 --> 0.260882).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 1.024589 \tValidation Loss: 0.259442\n",
      "Validation loss decreased (0.260882 --> 0.259442).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 1.023008 \tValidation Loss: 0.260231\n",
      "Epoch: 96 \tTraining Loss: 1.020674 \tValidation Loss: 0.263836\n",
      "Epoch: 97 \tTraining Loss: 1.019476 \tValidation Loss: 0.260097\n",
      "Epoch: 98 \tTraining Loss: 1.018338 \tValidation Loss: 0.259039\n",
      "Validation loss decreased (0.259442 --> 0.259039).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 1.017352 \tValidation Loss: 0.257688\n",
      "Validation loss decreased (0.259039 --> 0.257688).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 1.015929 \tValidation Loss: 0.257802\n"
     ]
    }
   ],
   "source": [
    "#Training the GRU Model\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "valid_loss_min = numpy.Inf \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  model_gru.train() \n",
    "\n",
    "  for data, target in train_loader_gru:\n",
    "    target = target.type(torch.LongTensor)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer_gru.zero_grad()\n",
    "    output = model_gru(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer_gru.step()\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  model_gru.eval() \n",
    "\n",
    "  for data, target in valid_loader_gru:\n",
    "    target = target.type(torch.LongTensor)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model_gru(data.float())\n",
    "    loss = criterion(output, target)\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "  train_loss = train_loss/len(train_loader_gru.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_gru.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_gru.state_dict(), 'model_gru.pt')\n",
    "    valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model with the lowest validation loss\n",
    "\n",
    "model_gru.load_state_dict(torch.load('model_gru.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Accuracy: 43 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the GRU Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_gru:\n",
    "    embeddings, labels = data\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    model_gru.to(device)\n",
    "    outputs = model_gru(embeddings.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "print('GRU Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow of information is controlled and the problem of long delays is completely eliminated in the Gated RNN's, thus, the accuracy is lower as compared to RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "083c622fe7b5f13fdad7c228f055512c42d6ad6014523f09179fc6cdf6c182b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
