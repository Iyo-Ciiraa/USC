{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6xyabwiXxFG",
        "outputId": "73188a62-8c58-4f21-ce55-5599bae19129"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ira_d\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ira_d\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ira_d\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\ira_d\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas \n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yK13B7u8XxFM"
      },
      "outputs": [],
      "source": [
        "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNfm8biXxFO"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBWAD7vnXxFQ",
        "outputId": "4b26f484-798b-46a9-d156-9204e38bad4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_8380\\454685861.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pandas.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip')\n"
          ]
        }
      ],
      "source": [
        "#data = pandas.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip')\n",
        "data = pandas.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRdqBdn4XxFQ"
      },
      "source": [
        "## Keep Reviews and Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FTIU1Wf9XxFR"
      },
      "outputs": [],
      "source": [
        "#Remove null value rows and reset index\n",
        "data = data.dropna()\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "#Keep only review_body column and corresponding star_rating column\n",
        "data = data[['review_body', 'star_rating']]\n",
        "\n",
        "#Removing all non-integer star_rating\n",
        "data['star_rating'] = data['star_rating'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUrWJqfmXxFR"
      },
      "source": [
        " ## We select 20000 reviews randomly from each rating class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H-AjWdinXxFS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_8380\\4283767408.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dataset = dataset.append(X)\n",
            "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_8380\\4283767408.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dataset = dataset.append(X)\n",
            "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_8380\\4283767408.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dataset = dataset.append(X)\n",
            "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_8380\\4283767408.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dataset = dataset.append(X)\n",
            "C:\\Users\\ira_d\\AppData\\Local\\Temp\\ipykernel_8380\\4283767408.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dataset = dataset.append(X)\n"
          ]
        }
      ],
      "source": [
        "sample_size = 20000\n",
        "dataset = pandas.DataFrame()\n",
        "for i in data.star_rating.unique():\n",
        "    X = data[data.star_rating == i].sample(sample_size)\n",
        "    dataset = dataset.append(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2do0ziFXxFT"
      },
      "source": [
        "# Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN029xsDXxFU"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0fVL0XUXxFU",
        "outputId": "4f84e9bd-9868-4014-992c-c4e177f02520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average character length before cleaning:  188.78542\n",
            "Average character length after cleaning:  172.82712\n"
          ]
        }
      ],
      "source": [
        "X, Y = dataset['review_body'].tolist(), dataset['star_rating'].tolist()\n",
        "\n",
        "#Print the average character length of the reviews before cleaning\n",
        "character_length_bf_cl = 0\n",
        "for i in range(len(X)):\n",
        "  character_length_bf_cl += len(X[i])\n",
        "print('Average character length before cleaning: ', character_length_bf_cl/len(X))\n",
        "\n",
        "#Convert reviews to lower case\n",
        "X = list(map(lambda x: str(x).lower(), X))\n",
        "\n",
        "#Remove HTML and URLs from reviews\n",
        "X = list(map(lambda x: re.sub('<.*>', '', x), X))\n",
        "X = list(map(lambda x: re.sub(r'https?://\\S+', '', x), X))\n",
        "\n",
        "#Remove non-alphabetical characters\n",
        "X = list(map(lambda x: re.sub('[^a-z ]', '', x), X))\n",
        "\n",
        "#Remove extra spaces\n",
        "X = list(map(lambda x: re.sub(' +', ' ', x), X))\n",
        "\n",
        "#Expand contractions\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def decontraction(s):\n",
        "    for word in s.split(' '):\n",
        "        if word in contractions.keys():\n",
        "            s = re.sub(word, contractions[word], s)\n",
        "    return s\n",
        "X = list(map(decontraction, X))\n",
        "\n",
        "#Print the average character length of the reviews after cleaning\n",
        "character_length_af_cl = 0\n",
        "for i in range(len(X)):\n",
        "  character_length_af_cl += len(X[i])\n",
        "print('Average character length after cleaning: ', character_length_af_cl/len(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-11zcHnXxFV"
      },
      "source": [
        "## remove the stop words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb53QYUpXxFV",
        "outputId": "3bd745b0-4710-416a-e99e-8aae5bcfc55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average character length before pre-processing:  172.82712\n"
          ]
        }
      ],
      "source": [
        "#Print the average character length of the reviews before pre-processing\n",
        "character_length_bf_pp = 0\n",
        "for i in range(len(X)):\n",
        "  character_length_bf_pp += len(X[i])\n",
        "print('Average character length before pre-processing: ', character_length_bf_pp/len(X))\n",
        "\n",
        "# remove stop words\n",
        "stopWords =set(stopwords.words('english'))\n",
        "def rmstopWords(s):\n",
        "    wordlist = s.split(' ')\n",
        "    newlist = []\n",
        "    for word in wordlist:\n",
        "        if word not in stopWords:\n",
        "            newlist.append(word)\n",
        "    s = ' '.join(newlist)\n",
        "    return s\n",
        "\n",
        "X = list(map(rmstopWords, X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFb-5XL4XxFW"
      },
      "source": [
        "## perform lemmatization  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErMk4l6MXxFW",
        "outputId": "257ca451-5d93-4818-8339-43d417780f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average character length after pre-processing:  105.60931\n"
          ]
        }
      ],
      "source": [
        "# perform lemmatization\n",
        "wnl = WordNetLemmatizer()\n",
        "X = list(map(lambda x: ' '.join(map(wnl.lemmatize, x.split(' '))), X))\n",
        "\n",
        "#Print the average character length of the reviews after pre-processing\n",
        "character_length_af_pp = 0\n",
        "for i in range(len(X)):\n",
        "  character_length_af_pp += len(X[i])\n",
        "print('Average character length after pre-processing: ', character_length_af_pp/len(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMICNcPTXxFW"
      },
      "source": [
        "# TF-IDF Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A5od7nzKXxFX"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "#Splitting data into training and testing set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(tfidf, Y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp4ibGKoXxFX"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6JGzT74XxFX",
        "outputId": "db047cce-b913-4088-f856-263b833c1d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron Accuracy =  0.40675\n",
            "0.47701028763384423 , 0.5639116406056094 , 0.5168334849863512 \n",
            "\n",
            "0.3212962962962963 , 0.26624040920716113 , 0.2911888111888112 \n",
            "\n",
            "0.310298826040555 , 0.2896637608966376 , 0.29962643307999487 \n",
            "\n",
            "0.34429974868631485 , 0.3800756620428752 , 0.36130424358666996 \n",
            "\n",
            "0.5557851239669421 , 0.5273217348689047 , 0.5411794291462341 \n",
            "\n",
            "0.40286501741992176 , 0.40675 , 0.40324975338706837 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, Y_train)\n",
        "\n",
        "Y_test_predict = perceptron.predict(X_test)\n",
        "\n",
        "report = metrics.classification_report(Y_test, Y_test_predict, output_dict = True)\n",
        "\n",
        "print('Perceptron Accuracy = ', accuracy_score(Y_test, Y_test_predict))\n",
        "print()\n",
        "#print(report)\n",
        "print(report['1']['precision'], ',', report['1']['recall'], ',', report['1']['f1-score'], '\\n')\n",
        "print(report['2']['precision'], ',', report['2']['recall'], ',', report['2']['f1-score'], '\\n')\n",
        "print(report['3']['precision'], ',', report['3']['recall'], ',', report['3']['f1-score'], '\\n')\n",
        "print(report['4']['precision'], ',', report['4']['recall'], ',', report['4']['f1-score'], '\\n')\n",
        "print(report['5']['precision'], ',', report['5']['recall'], ',', report['5']['f1-score'], '\\n')\n",
        "print(report['weighted avg']['precision'], ',', report['weighted avg']['recall'], ',', report['weighted avg']['f1-score'], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCO0SPqPXxFX"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V74xQgZ7XxFY",
        "outputId": "64c1c94b-a4d1-4226-dba3-a01eac54b5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Accuracy =  0.4897\n",
            "0.5507308684436801 , 0.6358897989575577 , 0.590254578965557 \n",
            "\n",
            "0.3753120665742025 , 0.34603580562659847 , 0.3600798403193613 \n",
            "\n",
            "0.40551181102362205 , 0.3334993773349938 , 0.36599699330326635 \n",
            "\n",
            "0.44148791745859356 , 0.4100882723833543 , 0.4252092050209205 \n",
            "\n",
            "0.612442202606137 , 0.7140406763048273 , 0.6593506052720896 \n",
            "\n",
            "0.4782185496041765 , 0.4897 , 0.4816145060218447 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "svm = LinearSVC()\n",
        "svm.fit(X_train, Y_train)\n",
        "\n",
        "Y_test_predict = svm.predict(X_test)\n",
        "\n",
        "report = metrics.classification_report(Y_test, Y_test_predict, output_dict = True)\n",
        "\n",
        "print('SVM Accuracy = ', accuracy_score(Y_test, Y_test_predict))\n",
        "print()\n",
        "#print(report)\n",
        "print(report['1']['precision'], ',', report['1']['recall'], ',', report['1']['f1-score'], '\\n')\n",
        "print(report['2']['precision'], ',', report['2']['recall'], ',', report['2']['f1-score'], '\\n')\n",
        "print(report['3']['precision'], ',', report['3']['recall'], ',', report['3']['f1-score'], '\\n')\n",
        "print(report['4']['precision'], ',', report['4']['recall'], ',', report['4']['f1-score'], '\\n')\n",
        "print(report['5']['precision'], ',', report['5']['recall'], ',', report['5']['f1-score'], '\\n')\n",
        "print(report['weighted avg']['precision'], ',', report['weighted avg']['recall'], ',', report['weighted avg']['f1-score'], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnAV9okxXxFY"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eme3xs9jXxFY",
        "outputId": "6a346af6-ae7e-41c4-80f4-d68540453d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.58167694768106 , 0.6319185902208985 , 0.6057577920532953 \n",
            "\n",
            "0.3997326203208556 , 0.38235294117647056 , 0.3908496732026144 \n",
            "\n",
            "0.425891677675033 , 0.40149439601494397 , 0.41333333333333333 \n",
            "\n",
            "0.46303291958985426 , 0.43278688524590164 , 0.4473992960500586 \n",
            "\n",
            "0.6520947176684881 , 0.7017887772604754 , 0.676029741531925 \n",
            "\n",
            "0.5056805061252834 , 0.51165 , 0.5080584641884275 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "logistic = LogisticRegression(solver = 'saga')\n",
        "logistic.fit(X_train, Y_train)\n",
        "\n",
        "Y_test_predict = logistic.predict(X_test)\n",
        "\n",
        "report = metrics.classification_report(Y_test, Y_test_predict, output_dict = True)\n",
        "#print(report)\n",
        "print(report['1']['precision'], ',', report['1']['recall'], ',', report['1']['f1-score'], '\\n')\n",
        "print(report['2']['precision'], ',', report['2']['recall'], ',', report['2']['f1-score'], '\\n')\n",
        "print(report['3']['precision'], ',', report['3']['recall'], ',', report['3']['f1-score'], '\\n')\n",
        "print(report['4']['precision'], ',', report['4']['recall'], ',', report['4']['f1-score'], '\\n')\n",
        "print(report['5']['precision'], ',', report['5']['recall'], ',', report['5']['f1-score'], '\\n')\n",
        "print(report['weighted avg']['precision'], ',', report['weighted avg']['recall'], ',', report['weighted avg']['f1-score'], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkxTqi4PXxFY"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLqeLJaGXxFY",
        "outputId": "ac760af8-2f5a-4e74-8a54-52502ef08094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.58167694768106 , 0.6319185902208985 , 0.6057577920532953 \n",
            "\n",
            "0.3997326203208556 , 0.38235294117647056 , 0.3908496732026144 \n",
            "\n",
            "0.425891677675033 , 0.40149439601494397 , 0.41333333333333333 \n",
            "\n",
            "0.46303291958985426 , 0.43278688524590164 , 0.4473992960500586 \n",
            "\n",
            "0.6520947176684881 , 0.7017887772604754 , 0.676029741531925 \n",
            "\n",
            "0.5056805061252834 , 0.51165 , 0.5080584641884275 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, Y_train)\n",
        "\n",
        "Y_test_predict = mnb.predict(X_test)\n",
        "\n",
        "Y_test_predict = logistic.predict(X_test)\n",
        "\n",
        "report = metrics.classification_report(Y_test, Y_test_predict, output_dict = True)\n",
        "#print(report)\n",
        "print(report['1']['precision'], ',', report['1']['recall'], ',', report['1']['f1-score'], '\\n')\n",
        "print(report['2']['precision'], ',', report['2']['recall'], ',', report['2']['f1-score'], '\\n')\n",
        "print(report['3']['precision'], ',', report['3']['recall'], ',', report['3']['f1-score'], '\\n')\n",
        "print(report['4']['precision'], ',', report['4']['recall'], ',', report['4']['f1-score'], '\\n')\n",
        "print(report['5']['precision'], ',', report['5']['recall'], ',', report['5']['f1-score'], '\\n')\n",
        "print(report['weighted avg']['precision'], ',', report['weighted avg']['recall'], ',', report['weighted avg']['f1-score'], '\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "083c622fe7b5f13fdad7c228f055512c42d6ad6014523f09179fc6cdf6c182b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
